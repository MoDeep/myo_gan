{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/jupyter/inspace/sang-min/myo_proejct\n",
      "30\n",
      "./dataset_2018_05_16/1/\n",
      "15\n",
      " _ :  Tensor(\"input_7:0\", shape=(?, 100), dtype=float32)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2048)              206848    \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 2048)              8192      \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 2, 2, 256)         1179904   \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 2, 2, 256)         8         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_23 (LeakyReLU)   (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_13 (UpSampling (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 4, 4, 128)         295040    \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 4, 4, 128)         16        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)   (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_14 (UpSampling (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 8, 8, 64)          73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 8, 8, 64)          32        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)   (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_15 (UpSampling (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 16, 16, 32)        18464     \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 16, 16, 32)        64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_26 (LeakyReLU)   (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_16 (UpSampling (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 32, 32, 16)        4624      \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 32, 32, 16)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_27 (LeakyReLU)   (None, 32, 32, 16)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_17 (UpSampling (None, 64, 64, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 64, 64, 8)         1160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 64, 64, 8)         256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_28 (LeakyReLU)   (None, 64, 64, 8)         0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 64, 64, 8)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_18 (UpSampling (None, 128, 128, 8)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 128, 128, 1)       73        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 128, 128, 1)       0         \n",
      "=================================================================\n",
      "Total params: 1,788,601\n",
      "Trainable params: 1,784,253\n",
      "Non-trainable params: 4,348\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:133: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(input_shape=(128, 128,..., filters=32, kernel_initializer=<keras.ini..., padding=\"same\", kernel_size=(5, 5))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:134: UserWarning: Update your `AveragePooling2D` call to the Keras 2 API: `AveragePooling2D(padding=\"valid\", pool_size=(2, 2))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:139: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(input_shape=(64, 64, 3..., filters=64, kernel_initializer=<keras.ini..., padding=\"same\", kernel_size=(5, 5))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:140: UserWarning: Update your `AveragePooling2D` call to the Keras 2 API: `AveragePooling2D(padding=\"valid\", pool_size=(2, 2))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:145: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(input_shape=(32, 32, 6..., filters=128, kernel_initializer=<keras.ini..., padding=\"same\", kernel_size=(5, 5))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:146: UserWarning: Update your `AveragePooling2D` call to the Keras 2 API: `AveragePooling2D(padding=\"valid\", pool_size=(2, 2))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:151: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(input_shape=(16, 16, 1..., filters=256, kernel_initializer=<keras.ini..., padding=\"same\", kernel_size=(5, 5))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:152: UserWarning: Update your `AveragePooling2D` call to the Keras 2 API: `AveragePooling2D(padding=\"valid\", pool_size=(2, 2))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:157: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(input_shape=(8, 8, 256..., filters=512, kernel_initializer=<keras.ini..., padding=\"same\", kernel_size=(5, 5))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:158: UserWarning: Update your `AveragePooling2D` call to the Keras 2 API: `AveragePooling2D(padding=\"valid\", pool_size=(2, 2))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 128, 128, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 128, 128, 32)      832       \n",
      "_________________________________________________________________\n",
      "average_pooling2d_11 (Averag (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_29 (LeakyReLU)   (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 64, 64, 32)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 64, 64, 64)        51264     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_12 (Averag (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_30 (LeakyReLU)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 32, 32, 64)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_36 (Conv2D)           (None, 32, 32, 128)       204928    \n",
      "_________________________________________________________________\n",
      "average_pooling2d_13 (Averag (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_31 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 16, 16, 128)       64        \n",
      "_________________________________________________________________\n",
      "conv2d_37 (Conv2D)           (None, 16, 16, 256)       819456    \n",
      "_________________________________________________________________\n",
      "average_pooling2d_14 (Averag (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_32 (LeakyReLU)   (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 8, 8, 256)         32        \n",
      "_________________________________________________________________\n",
      "conv2d_38 (Conv2D)           (None, 8, 8, 512)         3277312   \n",
      "_________________________________________________________________\n",
      "average_pooling2d_15 (Averag (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_33 (LeakyReLU)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 4, 4, 512)         16        \n",
      "_________________________________________________________________\n",
      "conv2d_39 (Conv2D)           (None, 4, 4, 1)           2049      \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 4,356,354\n",
      "Trainable params: 4,356,106\n",
      "Non-trainable params: 248\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "model_5 (Model)              (None, 128, 128, 1)       1788601   \n",
      "_________________________________________________________________\n",
      "model_6 (Model)              (None, 1)                 4356354   \n",
      "=================================================================\n",
      "Total params: 6,144,955\n",
      "Trainable params: 6,140,359\n",
      "Non-trainable params: 4,596\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/engine/training.py:973: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 3.723963] [G loss: 2.961917]\n",
      "gan imaga2 :  (128, 128, 1)\n",
      "1 [D loss: 2.444046] [G loss: 4.407784]\n",
      "2 [D loss: 3.709734] [G loss: 9.071479]\n",
      "3 [D loss: 5.467647] [G loss: 15.198322]\n",
      "4 [D loss: 5.161254] [G loss: 12.237347]\n",
      "5 [D loss: 7.687214] [G loss: 9.965475]\n",
      "6 [D loss: 2.522442] [G loss: 12.842836]\n",
      "7 [D loss: 16.030071] [G loss: 9.998827]\n",
      "8 [D loss: 16.696527] [G loss: 6.228770]\n",
      "9 [D loss: 16.844757] [G loss: 13.503104]\n",
      "10 [D loss: 18.226791] [G loss: 15.052586]\n",
      "11 [D loss: 14.284543] [G loss: 0.941207]\n",
      "12 [D loss: 15.091835] [G loss: 0.535742]\n",
      "13 [D loss: 14.888617] [G loss: 1.990538]\n",
      "14 [D loss: 14.453547] [G loss: 9.987333]\n",
      "15 [D loss: 12.568217] [G loss: 6.345208]\n",
      "16 [D loss: 13.540942] [G loss: 2.119086]\n",
      "17 [D loss: 12.544095] [G loss: 1.522686]\n",
      "18 [D loss: 13.587290] [G loss: 0.176606]\n",
      "19 [D loss: 14.908887] [G loss: 1.242170]\n",
      "20 [D loss: 13.249952] [G loss: 0.602819]\n",
      "21 [D loss: 13.874027] [G loss: 0.309603]\n",
      "22 [D loss: 14.327305] [G loss: 1.446420]\n",
      "23 [D loss: 15.542087] [G loss: 1.206709]\n",
      "24 [D loss: 16.087263] [G loss: 0.895909]\n",
      "25 [D loss: 13.741112] [G loss: 1.214235]\n",
      "26 [D loss: 15.183249] [G loss: 0.649073]\n",
      "./dataset_2018_05_16/2/\n",
      "27 [D loss: 14.383296] [G loss: 1.100114]\n",
      "28 [D loss: 15.131233] [G loss: 0.847577]\n",
      "29 [D loss: 14.091490] [G loss: 0.678938]\n",
      "30 [D loss: 14.505115] [G loss: 1.225468]\n",
      "31 [D loss: 14.471638] [G loss: 1.368271]\n",
      "32 [D loss: 15.311530] [G loss: 0.767573]\n",
      "33 [D loss: 15.236249] [G loss: 1.123928]\n",
      "34 [D loss: 14.055450] [G loss: 0.860186]\n",
      "35 [D loss: 14.132429] [G loss: 0.946112]\n",
      "36 [D loss: 13.429945] [G loss: 0.696106]\n",
      "37 [D loss: 13.030790] [G loss: 0.982023]\n",
      "38 [D loss: 14.058124] [G loss: 1.224424]\n",
      "39 [D loss: 15.236198] [G loss: 1.036681]\n",
      "40 [D loss: 14.167100] [G loss: 1.329100]\n",
      "41 [D loss: 14.474422] [G loss: 0.286118]\n",
      "42 [D loss: 14.773066] [G loss: 0.913899]\n",
      "43 [D loss: 13.505674] [G loss: 1.214017]\n",
      "44 [D loss: 14.904108] [G loss: 0.840860]\n",
      "45 [D loss: 15.216400] [G loss: 0.579675]\n",
      "46 [D loss: 13.660823] [G loss: 0.374224]\n",
      "47 [D loss: 14.803103] [G loss: 0.773726]\n",
      "48 [D loss: 15.550513] [G loss: 0.610602]\n",
      "49 [D loss: 14.485985] [G loss: 0.637985]\n",
      "50 [D loss: 16.438139] [G loss: 0.845213]\n",
      "51 [D loss: 14.683524] [G loss: 0.930908]\n",
      "52 [D loss: 13.345124] [G loss: 0.521644]\n",
      "53 [D loss: 14.760211] [G loss: 0.963966]\n",
      "54 [D loss: 15.057078] [G loss: 0.674565]\n",
      "./dataset_2018_05_16/3/\n",
      "55 [D loss: 13.804888] [G loss: 0.196548]\n",
      "56 [D loss: 13.157929] [G loss: 0.892187]\n",
      "57 [D loss: 14.582168] [G loss: 0.656458]\n",
      "58 [D loss: 13.052838] [G loss: 0.595757]\n",
      "59 [D loss: 16.273979] [G loss: 1.006640]\n",
      "60 [D loss: 13.899980] [G loss: 0.572607]\n",
      "61 [D loss: 11.741859] [G loss: 0.312709]\n",
      "62 [D loss: 15.716635] [G loss: 0.391986]\n",
      "63 [D loss: 15.152266] [G loss: 0.385559]\n",
      "64 [D loss: 13.980086] [G loss: 0.963152]\n",
      "65 [D loss: 15.012415] [G loss: 0.910645]\n",
      "66 [D loss: 13.358816] [G loss: 1.250440]\n",
      "67 [D loss: 15.024446] [G loss: 1.445296]\n",
      "68 [D loss: 14.350448] [G loss: 0.655086]\n",
      "69 [D loss: 15.556208] [G loss: 0.423702]\n",
      "70 [D loss: 14.439307] [G loss: 1.151824]\n",
      "71 [D loss: 15.490785] [G loss: 1.363815]\n",
      "72 [D loss: 13.747940] [G loss: 0.566957]\n",
      "73 [D loss: 14.924549] [G loss: 0.326008]\n",
      "74 [D loss: 14.311564] [G loss: 0.744477]\n",
      "75 [D loss: 14.714925] [G loss: 1.286781]\n",
      "76 [D loss: 14.040193] [G loss: 0.472156]\n",
      "77 [D loss: 14.571971] [G loss: 0.225764]\n",
      "78 [D loss: 14.716249] [G loss: 0.926102]\n",
      "79 [D loss: 14.031401] [G loss: 0.930078]\n",
      "80 [D loss: 14.823255] [G loss: 1.042384]\n",
      "81 [D loss: 14.685817] [G loss: 0.260616]\n",
      "82 [D loss: 14.184542] [G loss: 0.806317]\n",
      "83 [D loss: 13.435996] [G loss: 0.679368]\n",
      "./dataset_2018_05_16/4/\n",
      "84 [D loss: 13.514482] [G loss: 0.808129]\n",
      "85 [D loss: 14.114992] [G loss: 0.902646]\n",
      "86 [D loss: 15.823384] [G loss: 0.180912]\n",
      "87 [D loss: 14.462582] [G loss: 0.383382]\n",
      "88 [D loss: 13.778116] [G loss: 1.068151]\n",
      "89 [D loss: 15.340427] [G loss: 0.411872]\n",
      "90 [D loss: 13.776329] [G loss: 1.303689]\n",
      "91 [D loss: 14.513638] [G loss: 0.593044]\n",
      "92 [D loss: 13.015673] [G loss: 0.887912]\n",
      "93 [D loss: 13.457151] [G loss: 0.470243]\n",
      "94 [D loss: 14.523300] [G loss: 0.720187]\n",
      "95 [D loss: 14.036870] [G loss: 0.986140]\n",
      "96 [D loss: 13.691216] [G loss: 0.539208]\n",
      "97 [D loss: 13.557107] [G loss: 0.679873]\n",
      "98 [D loss: 14.471897] [G loss: 0.724402]\n",
      "99 [D loss: 14.306338] [G loss: 0.633467]\n",
      "100 [D loss: 14.770091] [G loss: 0.384449]\n",
      "101 [D loss: 15.085713] [G loss: 0.249631]\n",
      "102 [D loss: 13.822449] [G loss: 0.952389]\n",
      "103 [D loss: 14.419590] [G loss: 0.957971]\n",
      "104 [D loss: 15.144347] [G loss: 0.513012]\n",
      "105 [D loss: 15.608889] [G loss: 1.264891]\n",
      "106 [D loss: 14.444856] [G loss: 0.926988]\n",
      "107 [D loss: 13.719248] [G loss: 1.222328]\n",
      "108 [D loss: 13.221227] [G loss: 0.563721]\n",
      "109 [D loss: 13.373657] [G loss: 0.891026]\n",
      "110 [D loss: 14.762010] [G loss: 1.018632]\n",
      "111 [D loss: 15.026340] [G loss: 0.596074]\n",
      "./dataset_2018_05_16/5/\n",
      "112 [D loss: 14.175930] [G loss: 0.575772]\n",
      "113 [D loss: 13.013909] [G loss: 1.121920]\n",
      "114 [D loss: 13.571179] [G loss: 1.215356]\n",
      "115 [D loss: 14.098535] [G loss: 0.721778]\n",
      "116 [D loss: 12.761505] [G loss: 1.002097]\n",
      "117 [D loss: 14.086074] [G loss: 0.751249]\n",
      "118 [D loss: 15.104913] [G loss: 0.612434]\n",
      "119 [D loss: 14.830510] [G loss: 1.109026]\n",
      "120 [D loss: 14.587080] [G loss: 0.498259]\n",
      "121 [D loss: 15.804240] [G loss: 0.313497]\n",
      "122 [D loss: 14.188642] [G loss: 0.639109]\n",
      "123 [D loss: 14.582434] [G loss: 0.980297]\n",
      "124 [D loss: 15.752792] [G loss: 0.580922]\n",
      "125 [D loss: 12.688304] [G loss: 1.048178]\n",
      "126 [D loss: 14.057926] [G loss: 0.652516]\n",
      "127 [D loss: 12.791440] [G loss: 1.445092]\n",
      "128 [D loss: 14.106436] [G loss: 0.671877]\n",
      "129 [D loss: 14.050510] [G loss: 0.567796]\n",
      "130 [D loss: 15.647682] [G loss: 0.505073]\n",
      "131 [D loss: 12.381249] [G loss: 0.876126]\n",
      "132 [D loss: 15.204942] [G loss: 0.834818]\n",
      "133 [D loss: 13.929286] [G loss: 0.614105]\n",
      "134 [D loss: 14.744443] [G loss: 0.839577]\n",
      "135 [D loss: 12.330267] [G loss: 0.249583]\n",
      "136 [D loss: 14.486356] [G loss: 0.963482]\n",
      "137 [D loss: 15.599798] [G loss: 0.921100]\n",
      "138 [D loss: 14.579998] [G loss: 0.706245]\n",
      "139 [D loss: 14.072754] [G loss: 1.040563]\n",
      "./dataset_2018_05_16/6/\n",
      "140 [D loss: 14.861987] [G loss: 0.924230]\n",
      "141 [D loss: 14.800006] [G loss: 0.822659]\n",
      "142 [D loss: 15.731991] [G loss: 1.170165]\n",
      "143 [D loss: 13.336677] [G loss: 0.652111]\n",
      "144 [D loss: 14.374786] [G loss: 0.397089]\n",
      "145 [D loss: 15.508451] [G loss: 0.082565]\n",
      "146 [D loss: 14.965027] [G loss: 0.546453]\n",
      "147 [D loss: 14.243904] [G loss: 1.047883]\n",
      "148 [D loss: 14.923749] [G loss: 1.001229]\n",
      "149 [D loss: 14.049209] [G loss: 0.341465]\n",
      "150 [D loss: 14.060083] [G loss: 0.363764]\n",
      "151 [D loss: 14.918643] [G loss: 0.669796]\n",
      "152 [D loss: 13.181509] [G loss: 0.922717]\n",
      "153 [D loss: 14.689929] [G loss: 0.635497]\n",
      "154 [D loss: 14.299507] [G loss: 1.350893]\n",
      "155 [D loss: 13.244861] [G loss: 0.691487]\n",
      "156 [D loss: 13.264777] [G loss: 1.035529]\n",
      "157 [D loss: 13.889792] [G loss: 0.857672]\n",
      "158 [D loss: 15.349791] [G loss: 0.729487]\n",
      "159 [D loss: 14.953722] [G loss: 0.784186]\n",
      "160 [D loss: 13.603488] [G loss: 1.019394]\n",
      "161 [D loss: 15.818644] [G loss: 1.281773]\n",
      "162 [D loss: 15.272394] [G loss: 0.813275]\n",
      "163 [D loss: 15.348527] [G loss: 0.742656]\n",
      "164 [D loss: 15.616564] [G loss: 1.037225]\n",
      "165 [D loss: 14.244844] [G loss: 1.345266]\n",
      "166 [D loss: 13.460899] [G loss: 0.692888]\n",
      "167 [D loss: 14.160367] [G loss: 0.829087]\n",
      "./dataset_2018_05_16/7/\n",
      "168 [D loss: 13.857831] [G loss: 0.686762]\n",
      "169 [D loss: 15.236739] [G loss: 0.566847]\n",
      "170 [D loss: 13.526439] [G loss: 0.716219]\n",
      "171 [D loss: 12.759795] [G loss: 0.663889]\n",
      "172 [D loss: 14.708357] [G loss: 1.097340]\n",
      "173 [D loss: 13.479918] [G loss: 0.755801]\n",
      "174 [D loss: 13.922230] [G loss: 0.743160]\n",
      "175 [D loss: 12.649790] [G loss: 0.294233]\n",
      "176 [D loss: 15.697815] [G loss: 0.825219]\n",
      "177 [D loss: 15.085233] [G loss: 0.518530]\n",
      "178 [D loss: 13.754961] [G loss: 0.937844]\n",
      "179 [D loss: 14.861963] [G loss: 0.218321]\n",
      "180 [D loss: 13.586998] [G loss: 0.618874]\n",
      "181 [D loss: 14.021317] [G loss: 1.004599]\n",
      "182 [D loss: 14.569685] [G loss: 0.434325]\n",
      "183 [D loss: 15.157520] [G loss: 0.702472]\n",
      "184 [D loss: 14.504168] [G loss: 0.475279]\n",
      "185 [D loss: 14.642175] [G loss: 0.331163]\n",
      "186 [D loss: 14.732220] [G loss: 0.492505]\n",
      "187 [D loss: 13.745211] [G loss: 1.284559]\n",
      "188 [D loss: 16.067039] [G loss: 0.678131]\n",
      "189 [D loss: 14.098529] [G loss: 0.870362]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190 [D loss: 14.916066] [G loss: 0.770641]\n",
      "191 [D loss: 13.229236] [G loss: 0.865663]\n",
      "192 [D loss: 13.815040] [G loss: 0.803337]\n",
      "193 [D loss: 15.048456] [G loss: 1.075915]\n",
      "194 [D loss: 11.558144] [G loss: 1.201610]\n",
      "195 [D loss: 15.526395] [G loss: 0.627596]\n",
      "./dataset_2018_05_16/8/\n",
      "196 [D loss: 15.828815] [G loss: 0.901892]\n",
      "197 [D loss: 15.223315] [G loss: 0.977365]\n",
      "198 [D loss: 15.500110] [G loss: 0.758829]\n",
      "199 [D loss: 14.574747] [G loss: 0.795245]\n",
      "200 [D loss: 13.063581] [G loss: 1.034559]\n",
      "201 [D loss: 13.528203] [G loss: 0.427476]\n",
      "202 [D loss: 14.101253] [G loss: 0.490509]\n",
      "203 [D loss: 14.668740] [G loss: 0.803318]\n",
      "204 [D loss: 14.236229] [G loss: 0.645118]\n",
      "205 [D loss: 14.291060] [G loss: 1.576258]\n",
      "206 [D loss: 14.127821] [G loss: 0.743658]\n",
      "207 [D loss: 13.026061] [G loss: 1.239442]\n",
      "208 [D loss: 14.578290] [G loss: 1.405123]\n",
      "209 [D loss: 14.340516] [G loss: 0.788044]\n",
      "210 [D loss: 13.285287] [G loss: 1.053571]\n",
      "211 [D loss: 14.843405] [G loss: 0.398048]\n",
      "212 [D loss: 14.432972] [G loss: 0.426594]\n",
      "213 [D loss: 14.805729] [G loss: 1.024084]\n",
      "214 [D loss: 14.475740] [G loss: 0.955423]\n",
      "215 [D loss: 14.672165] [G loss: 1.123085]\n",
      "216 [D loss: 14.188004] [G loss: -0.145321]\n",
      "217 [D loss: 14.360691] [G loss: 1.272778]\n",
      "218 [D loss: 14.891561] [G loss: 0.521697]\n",
      "219 [D loss: 13.424559] [G loss: 0.699400]\n",
      "220 [D loss: 15.076225] [G loss: 0.613814]\n",
      "221 [D loss: 14.506889] [G loss: 1.026731]\n",
      "222 [D loss: 14.443870] [G loss: 0.855270]\n",
      "223 [D loss: 15.058920] [G loss: 0.694723]\n",
      "./dataset_2018_05_16/9/\n",
      "224 [D loss: 14.468053] [G loss: 0.507269]\n",
      "225 [D loss: 14.266891] [G loss: 0.654473]\n",
      "226 [D loss: 14.548310] [G loss: 0.734757]\n",
      "227 [D loss: 14.433673] [G loss: 1.226360]\n",
      "228 [D loss: 13.921479] [G loss: 0.636803]\n",
      "229 [D loss: 14.946317] [G loss: 0.739109]\n",
      "230 [D loss: 14.478971] [G loss: 0.853884]\n",
      "231 [D loss: 14.209793] [G loss: 0.579580]\n",
      "232 [D loss: 13.820670] [G loss: 0.548249]\n",
      "233 [D loss: 14.283072] [G loss: 0.557430]\n",
      "234 [D loss: 15.563557] [G loss: 1.199297]\n",
      "235 [D loss: 14.297425] [G loss: 1.219245]\n",
      "236 [D loss: 13.530526] [G loss: 0.802614]\n",
      "237 [D loss: 14.357833] [G loss: 0.180529]\n",
      "238 [D loss: 14.342622] [G loss: 0.301522]\n",
      "239 [D loss: 14.856656] [G loss: 0.923312]\n",
      "240 [D loss: 12.663525] [G loss: 0.481991]\n",
      "241 [D loss: 14.464960] [G loss: 0.651796]\n",
      "242 [D loss: 13.372671] [G loss: 0.392915]\n",
      "243 [D loss: 14.058211] [G loss: 1.115145]\n",
      "244 [D loss: 15.445127] [G loss: 0.590496]\n",
      "245 [D loss: 14.232935] [G loss: 0.739001]\n",
      "246 [D loss: 14.457257] [G loss: 0.790468]\n",
      "247 [D loss: 13.341896] [G loss: 0.843254]\n",
      "248 [D loss: 15.648977] [G loss: 1.085052]\n",
      "249 [D loss: 14.415536] [G loss: 0.684718]\n",
      "250 [D loss: 13.880393] [G loss: 0.637792]\n",
      "251 [D loss: 14.112043] [G loss: 1.243680]\n",
      "./dataset_2018_05_16/10/\n",
      "252 [D loss: 14.794616] [G loss: 0.489990]\n",
      "253 [D loss: 14.431145] [G loss: 1.106255]\n",
      "254 [D loss: 14.342772] [G loss: 0.607991]\n",
      "255 [D loss: 14.823482] [G loss: 1.037700]\n",
      "256 [D loss: 15.239414] [G loss: 1.232239]\n",
      "257 [D loss: 14.548181] [G loss: 1.051527]\n",
      "258 [D loss: 16.049593] [G loss: 1.058236]\n",
      "259 [D loss: 12.337732] [G loss: 0.942322]\n",
      "260 [D loss: 12.193954] [G loss: 0.549118]\n",
      "261 [D loss: 13.850314] [G loss: 1.063510]\n",
      "262 [D loss: 14.249699] [G loss: 0.637399]\n",
      "263 [D loss: 14.315019] [G loss: 0.912675]\n",
      "264 [D loss: 15.457393] [G loss: 0.978926]\n",
      "265 [D loss: 15.398086] [G loss: 1.297860]\n",
      "266 [D loss: 14.480732] [G loss: 0.756279]\n",
      "267 [D loss: 14.090550] [G loss: 1.036211]\n",
      "268 [D loss: 14.347133] [G loss: 0.300746]\n",
      "269 [D loss: 13.509616] [G loss: 0.429251]\n",
      "270 [D loss: 15.021007] [G loss: 0.816734]\n",
      "271 [D loss: 13.989151] [G loss: 0.935757]\n",
      "272 [D loss: 15.618269] [G loss: 1.100080]\n",
      "273 [D loss: 13.775253] [G loss: 0.809358]\n",
      "274 [D loss: 14.699803] [G loss: 0.774168]\n",
      "275 [D loss: 13.471206] [G loss: 0.597116]\n",
      "276 [D loss: 14.695635] [G loss: 1.049510]\n",
      "277 [D loss: 13.284796] [G loss: 0.540698]\n",
      "278 [D loss: 15.619196] [G loss: 0.696367]\n",
      "279 [D loss: 13.501077] [G loss: 0.838121]\n",
      "./dataset_2018_05_16/11/\n",
      "280 [D loss: 14.548868] [G loss: 0.794365]\n",
      "281 [D loss: 15.151155] [G loss: 1.170194]\n",
      "282 [D loss: 14.512095] [G loss: 0.807312]\n",
      "283 [D loss: 15.012310] [G loss: 0.747865]\n",
      "284 [D loss: 14.574928] [G loss: 0.641938]\n",
      "285 [D loss: 14.897303] [G loss: 0.407710]\n",
      "286 [D loss: 14.819511] [G loss: 0.583278]\n",
      "287 [D loss: 13.941859] [G loss: 0.332900]\n",
      "288 [D loss: 13.894447] [G loss: 1.187726]\n",
      "289 [D loss: 14.128529] [G loss: 0.350316]\n",
      "290 [D loss: 12.908055] [G loss: 1.154171]\n",
      "291 [D loss: 13.603674] [G loss: 0.856688]\n",
      "292 [D loss: 13.805971] [G loss: 0.865004]\n",
      "293 [D loss: 13.486582] [G loss: 0.873586]\n",
      "294 [D loss: 14.640115] [G loss: 0.137241]\n",
      "295 [D loss: 14.563515] [G loss: 0.623966]\n",
      "296 [D loss: 15.937877] [G loss: 0.382006]\n",
      "297 [D loss: 13.393295] [G loss: 1.235949]\n",
      "298 [D loss: 14.850889] [G loss: 0.450530]\n",
      "299 [D loss: 13.564139] [G loss: 0.864324]\n",
      "300 [D loss: 15.202820] [G loss: 0.820861]\n",
      "301 [D loss: 14.939204] [G loss: 0.711333]\n",
      "302 [D loss: 14.838629] [G loss: 0.262220]\n",
      "303 [D loss: 14.714822] [G loss: 0.719596]\n",
      "304 [D loss: 15.376343] [G loss: 0.728232]\n",
      "305 [D loss: 13.792175] [G loss: 0.758463]\n",
      "306 [D loss: 13.660281] [G loss: 0.886879]\n",
      "307 [D loss: 15.128668] [G loss: 0.504110]\n",
      "308 [D loss: 13.232389] [G loss: 0.636254]\n",
      "./dataset_2018_05_16/12/\n",
      "309 [D loss: 14.519146] [G loss: 0.599762]\n",
      "310 [D loss: 15.135685] [G loss: 0.749935]\n",
      "311 [D loss: 15.228833] [G loss: 0.947532]\n",
      "312 [D loss: 14.740837] [G loss: 1.422084]\n",
      "313 [D loss: 14.789877] [G loss: 0.775733]\n",
      "314 [D loss: 16.106930] [G loss: 0.719623]\n",
      "315 [D loss: 15.586463] [G loss: 0.730189]\n",
      "316 [D loss: 13.971760] [G loss: 1.075449]\n",
      "317 [D loss: 14.371913] [G loss: 1.029179]\n",
      "318 [D loss: 14.696523] [G loss: 0.385398]\n",
      "319 [D loss: 12.051279] [G loss: -0.421201]\n",
      "320 [D loss: 15.399201] [G loss: 0.623838]\n",
      "321 [D loss: 12.909249] [G loss: 1.197188]\n",
      "322 [D loss: 15.055417] [G loss: 0.965114]\n",
      "323 [D loss: 14.239253] [G loss: 0.934801]\n",
      "324 [D loss: 14.432287] [G loss: 0.984434]\n",
      "325 [D loss: 12.182629] [G loss: 1.149138]\n",
      "326 [D loss: 14.846762] [G loss: 0.700463]\n",
      "327 [D loss: 14.213318] [G loss: 0.301894]\n",
      "328 [D loss: 15.163428] [G loss: 0.392232]\n",
      "329 [D loss: 15.086553] [G loss: 0.700657]\n",
      "330 [D loss: 15.387356] [G loss: 0.600299]\n",
      "331 [D loss: 13.167792] [G loss: 1.186666]\n",
      "332 [D loss: 15.261921] [G loss: 0.555252]\n",
      "333 [D loss: 14.832300] [G loss: 0.902936]\n",
      "334 [D loss: 14.743239] [G loss: 0.567904]\n",
      "335 [D loss: 14.281518] [G loss: 0.895118]\n",
      "336 [D loss: 13.576781] [G loss: 0.907566]\n",
      "./dataset_2018_05_16/13/\n",
      "337 [D loss: 14.654792] [G loss: 0.886604]\n",
      "338 [D loss: 14.876724] [G loss: 0.287574]\n",
      "339 [D loss: 13.850515] [G loss: 0.785307]\n",
      "340 [D loss: 14.269432] [G loss: 0.530379]\n",
      "341 [D loss: 14.372058] [G loss: 1.113412]\n",
      "342 [D loss: 13.874190] [G loss: 0.929425]\n",
      "343 [D loss: 14.548169] [G loss: 1.113961]\n",
      "344 [D loss: 16.009474] [G loss: 1.277848]\n",
      "345 [D loss: 14.225599] [G loss: 1.167131]\n",
      "346 [D loss: 12.473509] [G loss: 1.007093]\n",
      "347 [D loss: 13.577641] [G loss: 0.701244]\n",
      "348 [D loss: 13.929615] [G loss: 0.416814]\n",
      "349 [D loss: 13.555999] [G loss: 0.884158]\n",
      "350 [D loss: 15.271535] [G loss: 0.122913]\n",
      "351 [D loss: 15.749975] [G loss: 0.615850]\n",
      "352 [D loss: 14.030900] [G loss: 0.844214]\n",
      "353 [D loss: 14.630771] [G loss: 0.821368]\n",
      "354 [D loss: 14.333528] [G loss: 0.811385]\n",
      "355 [D loss: 14.896853] [G loss: 0.630281]\n",
      "356 [D loss: 13.794437] [G loss: 0.775124]\n",
      "357 [D loss: 14.904410] [G loss: 0.438352]\n",
      "358 [D loss: 14.067294] [G loss: 1.030882]\n",
      "359 [D loss: 15.452915] [G loss: 1.060903]\n",
      "360 [D loss: 14.088596] [G loss: 0.644084]\n",
      "361 [D loss: 15.196200] [G loss: 1.006043]\n",
      "362 [D loss: 14.488657] [G loss: 0.588123]\n",
      "363 [D loss: 15.192261] [G loss: 0.382064]\n",
      "364 [D loss: 13.149014] [G loss: 0.819450]\n",
      "./dataset_2018_05_16/14/\n",
      "365 [D loss: 13.148351] [G loss: 1.300303]\n",
      "366 [D loss: 14.723131] [G loss: 0.921868]\n",
      "367 [D loss: 13.598495] [G loss: 0.314079]\n",
      "368 [D loss: 13.079791] [G loss: 0.897683]\n",
      "369 [D loss: 14.531760] [G loss: 1.144595]\n",
      "370 [D loss: 13.718566] [G loss: 0.584127]\n",
      "371 [D loss: 15.830750] [G loss: 0.645017]\n",
      "372 [D loss: 12.791336] [G loss: 0.893349]\n",
      "373 [D loss: 15.307167] [G loss: 0.360757]\n",
      "374 [D loss: 13.977873] [G loss: 0.289781]\n",
      "375 [D loss: 13.300649] [G loss: 1.093459]\n",
      "376 [D loss: 14.987997] [G loss: 1.036031]\n",
      "377 [D loss: 13.530240] [G loss: 0.842150]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378 [D loss: 13.305244] [G loss: 0.665187]\n",
      "379 [D loss: 13.670847] [G loss: 1.003670]\n",
      "380 [D loss: 15.560558] [G loss: 0.680465]\n",
      "381 [D loss: 14.361770] [G loss: 0.816247]\n",
      "382 [D loss: 13.804301] [G loss: 0.395254]\n",
      "383 [D loss: 14.284969] [G loss: 0.109584]\n",
      "384 [D loss: 15.391670] [G loss: 0.530823]\n",
      "385 [D loss: 15.679436] [G loss: 0.569431]\n",
      "386 [D loss: 13.975462] [G loss: 1.152106]\n",
      "387 [D loss: 14.554778] [G loss: 0.853918]\n",
      "388 [D loss: 13.814711] [G loss: 0.781304]\n",
      "389 [D loss: 14.664603] [G loss: 0.913771]\n",
      "390 [D loss: 12.359149] [G loss: 1.115541]\n",
      "391 [D loss: 15.432719] [G loss: 0.785514]\n",
      "392 [D loss: 14.026072] [G loss: 0.024571]\n",
      "./dataset_2018_05_16/15/\n",
      "393 [D loss: 14.288643] [G loss: 0.754074]\n",
      "394 [D loss: 15.113220] [G loss: 1.222590]\n",
      "395 [D loss: 14.545771] [G loss: 0.433413]\n",
      "396 [D loss: 14.975035] [G loss: 0.987359]\n",
      "397 [D loss: 14.402120] [G loss: 1.269862]\n",
      "398 [D loss: 13.696787] [G loss: 1.029140]\n",
      "399 [D loss: 14.732074] [G loss: 0.712607]\n",
      "400 [D loss: 14.772863] [G loss: 0.739782]\n",
      "401 [D loss: 15.693881] [G loss: 0.380266]\n",
      "402 [D loss: 13.737111] [G loss: 1.301553]\n",
      "403 [D loss: 14.354980] [G loss: 0.995903]\n",
      "404 [D loss: 12.961486] [G loss: 1.124815]\n",
      "405 [D loss: 13.392934] [G loss: 0.795456]\n",
      "406 [D loss: 12.991348] [G loss: 0.957357]\n",
      "407 [D loss: 14.377729] [G loss: 0.518720]\n",
      "408 [D loss: 15.301905] [G loss: 0.837063]\n",
      "409 [D loss: 12.919176] [G loss: 0.912985]\n",
      "410 [D loss: 13.109105] [G loss: 1.045648]\n",
      "411 [D loss: 14.044290] [G loss: 0.950936]\n",
      "412 [D loss: 14.058451] [G loss: 0.552946]\n",
      "413 [D loss: 15.260153] [G loss: 1.226671]\n",
      "414 [D loss: 13.970690] [G loss: 0.956393]\n",
      "415 [D loss: 14.222722] [G loss: 0.321904]\n",
      "416 [D loss: 13.181850] [G loss: 0.749347]\n",
      "417 [D loss: 15.551500] [G loss: 0.240631]\n",
      "418 [D loss: 14.006208] [G loss: 0.445681]\n",
      "419 [D loss: 13.594169] [G loss: 0.913915]\n",
      "420 [D loss: 13.713770] [G loss: 0.955120]\n",
      "./dataset_2018_05_16/1/\n",
      "421 [D loss: 13.734078] [G loss: 0.834383]\n",
      "422 [D loss: 14.175854] [G loss: 1.054139]\n",
      "423 [D loss: 15.073998] [G loss: 0.654972]\n",
      "424 [D loss: 14.804420] [G loss: 0.275472]\n",
      "425 [D loss: 14.676108] [G loss: 0.914442]\n",
      "426 [D loss: 14.405849] [G loss: 1.291875]\n",
      "427 [D loss: 14.412664] [G loss: 0.908958]\n",
      "428 [D loss: 15.213405] [G loss: 0.749971]\n",
      "429 [D loss: 14.740549] [G loss: 1.121511]\n",
      "430 [D loss: 13.747687] [G loss: 0.918983]\n",
      "431 [D loss: 14.743480] [G loss: 1.306669]\n",
      "432 [D loss: 14.457480] [G loss: 0.538942]\n",
      "433 [D loss: 14.599491] [G loss: 0.827406]\n",
      "434 [D loss: 13.947631] [G loss: 0.344101]\n",
      "435 [D loss: 14.687844] [G loss: 0.527429]\n",
      "436 [D loss: 14.206481] [G loss: 0.655631]\n",
      "437 [D loss: 14.668732] [G loss: 0.165488]\n",
      "438 [D loss: 13.817428] [G loss: 0.663225]\n",
      "439 [D loss: 14.168495] [G loss: 0.386469]\n",
      "440 [D loss: 15.334971] [G loss: 0.723007]\n",
      "441 [D loss: 14.311869] [G loss: 1.050902]\n",
      "442 [D loss: 15.035821] [G loss: 0.832994]\n",
      "443 [D loss: 13.102468] [G loss: 1.093048]\n",
      "444 [D loss: 13.602441] [G loss: 0.495613]\n",
      "445 [D loss: 14.188002] [G loss: 0.877552]\n",
      "446 [D loss: 14.366229] [G loss: 0.740629]\n",
      "447 [D loss: 15.181220] [G loss: 1.373960]\n",
      "448 [D loss: 15.737987] [G loss: 0.692670]\n",
      "./dataset_2018_05_16/2/\n",
      "449 [D loss: 14.910869] [G loss: 1.086301]\n",
      "450 [D loss: 14.019682] [G loss: 0.690724]\n",
      "451 [D loss: 13.510009] [G loss: 1.022745]\n",
      "452 [D loss: 15.004365] [G loss: 1.260736]\n",
      "453 [D loss: 12.393699] [G loss: 0.508909]\n",
      "454 [D loss: 13.627152] [G loss: 0.576760]\n",
      "455 [D loss: 14.591127] [G loss: 0.569666]\n",
      "456 [D loss: 14.198061] [G loss: 1.034128]\n",
      "457 [D loss: 14.164841] [G loss: 0.383862]\n",
      "458 [D loss: 14.962027] [G loss: 0.357629]\n",
      "459 [D loss: 14.371601] [G loss: 0.495988]\n",
      "460 [D loss: 14.860199] [G loss: 0.596207]\n",
      "461 [D loss: 14.793929] [G loss: 0.616232]\n",
      "462 [D loss: 13.885417] [G loss: 1.118093]\n",
      "463 [D loss: 16.217766] [G loss: 0.570113]\n",
      "464 [D loss: 15.466786] [G loss: 0.651367]\n",
      "465 [D loss: 14.801008] [G loss: 0.429440]\n",
      "466 [D loss: 13.950521] [G loss: 0.578132]\n",
      "467 [D loss: 14.008132] [G loss: 0.675684]\n",
      "468 [D loss: 14.495881] [G loss: 1.281553]\n",
      "469 [D loss: 13.480373] [G loss: 0.801998]\n",
      "470 [D loss: 14.947067] [G loss: 0.779348]\n",
      "471 [D loss: 14.541147] [G loss: 1.132159]\n",
      "472 [D loss: 14.468348] [G loss: 0.980306]\n",
      "473 [D loss: 14.368351] [G loss: 0.745421]\n",
      "474 [D loss: 15.881500] [G loss: 0.868073]\n",
      "475 [D loss: 14.951314] [G loss: 0.546382]\n",
      "476 [D loss: 13.967154] [G loss: 1.305515]\n",
      "./dataset_2018_05_16/3/\n",
      "477 [D loss: 14.514311] [G loss: 0.420435]\n",
      "478 [D loss: 15.446424] [G loss: 0.919373]\n",
      "479 [D loss: 13.387307] [G loss: 0.560455]\n",
      "480 [D loss: 14.445579] [G loss: 0.882827]\n",
      "481 [D loss: 14.421797] [G loss: 0.505905]\n",
      "482 [D loss: 15.156395] [G loss: 0.542188]\n",
      "483 [D loss: 14.406610] [G loss: 1.235525]\n",
      "484 [D loss: 13.106960] [G loss: 1.181378]\n",
      "485 [D loss: 13.968855] [G loss: 0.586879]\n",
      "486 [D loss: 14.384390] [G loss: 0.656657]\n",
      "487 [D loss: 14.505314] [G loss: 0.574471]\n",
      "488 [D loss: 14.472157] [G loss: 0.695431]\n",
      "489 [D loss: 14.528518] [G loss: 0.707446]\n",
      "490 [D loss: 15.194725] [G loss: 0.640320]\n",
      "491 [D loss: 12.976713] [G loss: 0.432774]\n",
      "492 [D loss: 13.494602] [G loss: 0.084184]\n",
      "493 [D loss: 15.163509] [G loss: 0.863011]\n",
      "494 [D loss: 14.765314] [G loss: 0.659819]\n",
      "495 [D loss: 13.571639] [G loss: 0.665784]\n",
      "496 [D loss: 13.940612] [G loss: 0.828281]\n",
      "497 [D loss: 14.913717] [G loss: 0.807323]\n",
      "498 [D loss: 14.448059] [G loss: 1.077359]\n",
      "499 [D loss: 11.933880] [G loss: 0.905770]\n",
      "500 [D loss: 14.791969] [G loss: 1.266154]\n",
      "gan imaga2 :  (128, 128, 1)\n",
      "501 [D loss: 13.381089] [G loss: 0.480181]\n",
      "502 [D loss: 12.552359] [G loss: 1.103391]\n",
      "503 [D loss: 13.851318] [G loss: 0.888386]\n",
      "504 [D loss: 14.599449] [G loss: 0.899132]\n",
      "./dataset_2018_05_16/4/\n",
      "505 [D loss: 13.917481] [G loss: 0.438387]\n",
      "506 [D loss: 13.289846] [G loss: 0.990321]\n",
      "507 [D loss: 13.992610] [G loss: 0.409396]\n",
      "508 [D loss: 14.035404] [G loss: 0.723077]\n",
      "509 [D loss: 15.337461] [G loss: 0.606999]\n",
      "510 [D loss: 13.689085] [G loss: 0.728718]\n",
      "511 [D loss: 14.970345] [G loss: 0.640299]\n",
      "512 [D loss: 14.033097] [G loss: 0.884887]\n",
      "513 [D loss: 14.712393] [G loss: 0.557622]\n",
      "514 [D loss: 12.401995] [G loss: 0.542764]\n",
      "515 [D loss: 13.800076] [G loss: 0.669070]\n",
      "516 [D loss: 15.135051] [G loss: 0.824757]\n",
      "517 [D loss: 14.078787] [G loss: 0.291789]\n",
      "518 [D loss: 14.198919] [G loss: 0.522926]\n",
      "519 [D loss: 14.992148] [G loss: 0.848416]\n",
      "520 [D loss: 14.085636] [G loss: 1.161527]\n",
      "521 [D loss: 13.730328] [G loss: 0.797799]\n",
      "522 [D loss: 14.425578] [G loss: 0.155882]\n",
      "523 [D loss: 16.400312] [G loss: 0.785325]\n",
      "524 [D loss: 14.954791] [G loss: 1.182877]\n",
      "525 [D loss: 14.309856] [G loss: 0.840872]\n",
      "526 [D loss: 14.195641] [G loss: 1.028850]\n",
      "527 [D loss: 14.108993] [G loss: 1.010177]\n",
      "528 [D loss: 17.114153] [G loss: 0.381002]\n",
      "529 [D loss: 13.335317] [G loss: 1.183789]\n",
      "530 [D loss: 13.849737] [G loss: 0.553199]\n",
      "531 [D loss: 13.561117] [G loss: 0.527424]\n",
      "532 [D loss: 14.832846] [G loss: 0.911124]\n",
      "./dataset_2018_05_16/5/\n",
      "533 [D loss: 14.205098] [G loss: 0.698381]\n",
      "534 [D loss: 15.213321] [G loss: 1.177589]\n",
      "535 [D loss: 14.094471] [G loss: 0.596181]\n",
      "536 [D loss: 13.167111] [G loss: 0.434572]\n",
      "537 [D loss: 15.381483] [G loss: 0.851132]\n",
      "538 [D loss: 15.482443] [G loss: 0.929174]\n",
      "539 [D loss: 13.925750] [G loss: 1.197199]\n",
      "540 [D loss: 13.662253] [G loss: 0.896617]\n",
      "541 [D loss: 14.169497] [G loss: 0.560322]\n",
      "542 [D loss: 13.047824] [G loss: 1.076189]\n",
      "543 [D loss: 12.854193] [G loss: 0.843052]\n",
      "544 [D loss: 13.960784] [G loss: 0.692226]\n",
      "545 [D loss: 16.157497] [G loss: 0.899233]\n",
      "546 [D loss: 14.108761] [G loss: 0.639195]\n",
      "547 [D loss: 14.902393] [G loss: 0.439721]\n",
      "548 [D loss: 15.151410] [G loss: 0.443795]\n",
      "549 [D loss: 14.313675] [G loss: 0.733972]\n",
      "550 [D loss: 14.427784] [G loss: 1.036632]\n",
      "551 [D loss: 13.478773] [G loss: 0.649799]\n",
      "552 [D loss: 14.519800] [G loss: 0.307573]\n",
      "553 [D loss: 13.976585] [G loss: 0.882049]\n",
      "554 [D loss: 14.116332] [G loss: 1.271999]\n",
      "555 [D loss: 15.842340] [G loss: 0.384567]\n",
      "556 [D loss: 14.368864] [G loss: 1.100929]\n",
      "557 [D loss: 15.134443] [G loss: 0.668387]\n",
      "558 [D loss: 14.119267] [G loss: 0.898897]\n",
      "559 [D loss: 13.894094] [G loss: 0.862838]\n",
      "560 [D loss: 14.255590] [G loss: 1.180436]\n",
      "./dataset_2018_05_16/6/\n",
      "561 [D loss: 12.954775] [G loss: 0.896476]\n",
      "562 [D loss: 14.588280] [G loss: 1.155425]\n",
      "563 [D loss: 13.333253] [G loss: 0.736528]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "564 [D loss: 13.857983] [G loss: 1.617778]\n",
      "565 [D loss: 14.296345] [G loss: 0.710032]\n",
      "566 [D loss: 14.731493] [G loss: 0.749676]\n",
      "567 [D loss: 15.322863] [G loss: 0.541302]\n",
      "568 [D loss: 13.206870] [G loss: 0.831386]\n",
      "569 [D loss: 12.512849] [G loss: 0.582180]\n",
      "570 [D loss: 15.590261] [G loss: 0.630350]\n",
      "571 [D loss: 14.373325] [G loss: 0.374079]\n",
      "572 [D loss: 15.456250] [G loss: 0.893720]\n",
      "573 [D loss: 12.912080] [G loss: 0.970816]\n",
      "574 [D loss: 13.735123] [G loss: 0.710316]\n",
      "575 [D loss: 14.344718] [G loss: 0.681356]\n",
      "576 [D loss: 13.973104] [G loss: 0.466488]\n",
      "577 [D loss: 14.675939] [G loss: 0.847457]\n",
      "578 [D loss: 13.787417] [G loss: 1.250385]\n",
      "579 [D loss: 14.607995] [G loss: 0.533908]\n",
      "580 [D loss: 15.168560] [G loss: 0.800752]\n",
      "581 [D loss: 15.279188] [G loss: 0.583539]\n",
      "582 [D loss: 14.475411] [G loss: 1.025184]\n",
      "583 [D loss: 12.887415] [G loss: 0.921663]\n",
      "584 [D loss: 14.920887] [G loss: 1.019215]\n",
      "585 [D loss: 14.830446] [G loss: 1.226311]\n",
      "586 [D loss: 14.398750] [G loss: 0.706352]\n",
      "587 [D loss: 13.272758] [G loss: 1.223385]\n",
      "588 [D loss: 14.116280] [G loss: 0.541834]\n",
      "./dataset_2018_05_16/7/\n",
      "589 [D loss: 12.192086] [G loss: 0.703017]\n",
      "590 [D loss: 14.165073] [G loss: 0.688749]\n",
      "591 [D loss: 14.360162] [G loss: 0.749584]\n",
      "592 [D loss: 14.222168] [G loss: 0.813330]\n",
      "593 [D loss: 14.838095] [G loss: 0.927621]\n",
      "594 [D loss: 14.318423] [G loss: 1.323466]\n",
      "595 [D loss: 15.191043] [G loss: 0.906018]\n",
      "596 [D loss: 13.423498] [G loss: 0.620550]\n",
      "597 [D loss: 15.201262] [G loss: 0.690368]\n",
      "598 [D loss: 14.737465] [G loss: 0.749467]\n",
      "599 [D loss: 14.385213] [G loss: 1.051302]\n",
      "600 [D loss: 14.994446] [G loss: 1.032743]\n",
      "601 [D loss: 15.912750] [G loss: 1.261153]\n",
      "602 [D loss: 13.852974] [G loss: 1.099983]\n",
      "603 [D loss: 15.160479] [G loss: 0.777361]\n",
      "604 [D loss: 14.927391] [G loss: 0.834099]\n",
      "605 [D loss: 13.982036] [G loss: 1.332590]\n",
      "606 [D loss: 14.960064] [G loss: 0.732342]\n",
      "607 [D loss: 14.399696] [G loss: 1.048553]\n",
      "608 [D loss: 14.876197] [G loss: 0.827999]\n",
      "609 [D loss: 15.682863] [G loss: 0.927557]\n",
      "610 [D loss: 13.655822] [G loss: 0.655212]\n",
      "611 [D loss: 14.347346] [G loss: 1.036085]\n",
      "612 [D loss: 13.993011] [G loss: 0.548417]\n",
      "613 [D loss: 14.343624] [G loss: 0.281462]\n",
      "614 [D loss: 12.559111] [G loss: 0.735595]\n",
      "615 [D loss: 14.407423] [G loss: 0.682268]\n",
      "616 [D loss: 12.768572] [G loss: 0.345732]\n",
      "617 [D loss: 15.418050] [G loss: 0.604091]\n",
      "./dataset_2018_05_16/8/\n",
      "618 [D loss: 14.410055] [G loss: 0.803875]\n",
      "619 [D loss: 15.320616] [G loss: 0.976393]\n",
      "620 [D loss: 14.436133] [G loss: 0.961847]\n",
      "621 [D loss: 15.418105] [G loss: 0.874765]\n",
      "622 [D loss: 14.968141] [G loss: 0.925035]\n",
      "623 [D loss: 16.104181] [G loss: 0.306596]\n",
      "624 [D loss: 13.695930] [G loss: 1.434959]\n",
      "625 [D loss: 15.108742] [G loss: 0.767332]\n",
      "626 [D loss: 15.414371] [G loss: 1.270604]\n",
      "627 [D loss: 15.444389] [G loss: 0.667701]\n",
      "628 [D loss: 14.656901] [G loss: 1.035778]\n",
      "629 [D loss: 14.090425] [G loss: 0.425325]\n",
      "630 [D loss: 15.079473] [G loss: 0.994356]\n",
      "631 [D loss: 14.343978] [G loss: 0.893903]\n",
      "632 [D loss: 13.429358] [G loss: 0.516162]\n",
      "633 [D loss: 14.770769] [G loss: 0.806563]\n",
      "634 [D loss: 13.493818] [G loss: 0.942547]\n",
      "635 [D loss: 14.275448] [G loss: 1.213695]\n",
      "636 [D loss: 15.052804] [G loss: 0.784080]\n",
      "637 [D loss: 13.255589] [G loss: 0.851609]\n",
      "638 [D loss: 13.578217] [G loss: 0.829579]\n",
      "639 [D loss: 15.916124] [G loss: 1.020841]\n",
      "640 [D loss: 15.617028] [G loss: 0.698305]\n",
      "641 [D loss: 15.296218] [G loss: 0.431379]\n",
      "642 [D loss: 15.010680] [G loss: 1.336679]\n",
      "643 [D loss: 15.144028] [G loss: 1.157539]\n",
      "644 [D loss: 14.871983] [G loss: 0.915931]\n",
      "645 [D loss: 14.513422] [G loss: 1.381531]\n",
      "./dataset_2018_05_16/9/\n",
      "646 [D loss: 14.753918] [G loss: 0.763047]\n",
      "647 [D loss: 12.754073] [G loss: 0.800477]\n",
      "648 [D loss: 14.194798] [G loss: 0.936270]\n",
      "649 [D loss: 15.746128] [G loss: 0.889441]\n",
      "650 [D loss: 14.912169] [G loss: 0.809656]\n",
      "651 [D loss: 13.427846] [G loss: 0.810798]\n",
      "652 [D loss: 14.472831] [G loss: 0.531298]\n",
      "653 [D loss: 15.163408] [G loss: 0.563790]\n",
      "654 [D loss: 16.083288] [G loss: 0.568031]\n",
      "655 [D loss: 13.338240] [G loss: 0.802947]\n",
      "656 [D loss: 16.171082] [G loss: 0.729482]\n",
      "657 [D loss: 14.176500] [G loss: 0.461549]\n",
      "658 [D loss: 15.920568] [G loss: 0.832588]\n",
      "659 [D loss: 14.065015] [G loss: 0.923910]\n",
      "660 [D loss: 14.937572] [G loss: 0.630098]\n",
      "661 [D loss: 14.207863] [G loss: 0.720509]\n",
      "662 [D loss: 14.928140] [G loss: 1.256290]\n",
      "663 [D loss: 13.988338] [G loss: 1.182660]\n",
      "664 [D loss: 14.791001] [G loss: 0.780568]\n",
      "665 [D loss: 15.263892] [G loss: 0.906373]\n",
      "666 [D loss: 15.768577] [G loss: 1.098077]\n",
      "667 [D loss: 13.956921] [G loss: 0.496890]\n",
      "668 [D loss: 14.878919] [G loss: 0.558659]\n",
      "669 [D loss: 13.482906] [G loss: 0.383563]\n",
      "670 [D loss: 14.966135] [G loss: 1.010362]\n",
      "671 [D loss: 15.567173] [G loss: 0.608501]\n",
      "672 [D loss: 15.716083] [G loss: 1.138050]\n",
      "673 [D loss: 14.514971] [G loss: 1.138508]\n",
      "./dataset_2018_05_16/10/\n",
      "674 [D loss: 15.550471] [G loss: 0.547562]\n",
      "675 [D loss: 14.651532] [G loss: 0.574672]\n",
      "676 [D loss: 14.429802] [G loss: 0.985412]\n",
      "677 [D loss: 15.095445] [G loss: 0.827405]\n",
      "678 [D loss: 14.832922] [G loss: 0.835635]\n",
      "679 [D loss: 15.088450] [G loss: 0.550427]\n",
      "680 [D loss: 15.920146] [G loss: 0.600163]\n",
      "681 [D loss: 14.009285] [G loss: 0.427666]\n",
      "682 [D loss: 14.805143] [G loss: 0.422552]\n",
      "683 [D loss: 14.038272] [G loss: 0.943081]\n",
      "684 [D loss: 13.742434] [G loss: 0.675542]\n",
      "685 [D loss: 13.617831] [G loss: 1.234456]\n",
      "686 [D loss: 15.393396] [G loss: 0.936094]\n",
      "687 [D loss: 14.539822] [G loss: 0.623460]\n",
      "688 [D loss: 13.087191] [G loss: 1.095545]\n",
      "689 [D loss: 15.747296] [G loss: 0.484631]\n",
      "690 [D loss: 13.524871] [G loss: 0.875860]\n",
      "691 [D loss: 15.055073] [G loss: 0.830368]\n",
      "692 [D loss: 13.298626] [G loss: 1.137408]\n",
      "693 [D loss: 14.387414] [G loss: 0.431619]\n",
      "694 [D loss: 15.703831] [G loss: 0.362293]\n",
      "695 [D loss: 15.086678] [G loss: 1.682995]\n",
      "696 [D loss: 15.289567] [G loss: 0.976288]\n",
      "697 [D loss: 14.439692] [G loss: 0.924408]\n",
      "698 [D loss: 14.985399] [G loss: 0.820347]\n",
      "699 [D loss: 13.467292] [G loss: 0.794673]\n",
      "700 [D loss: 13.753473] [G loss: 0.757326]\n",
      "701 [D loss: 12.625753] [G loss: 0.902883]\n",
      "./dataset_2018_05_16/11/\n",
      "702 [D loss: 15.721914] [G loss: 0.488280]\n",
      "703 [D loss: 15.021591] [G loss: 0.966223]\n",
      "704 [D loss: 14.511437] [G loss: 1.066633]\n",
      "705 [D loss: 13.575372] [G loss: 1.048754]\n",
      "706 [D loss: 14.359967] [G loss: 0.682352]\n",
      "707 [D loss: 14.703172] [G loss: 1.146045]\n",
      "708 [D loss: 14.687664] [G loss: 1.122352]\n",
      "709 [D loss: 14.568112] [G loss: 0.845210]\n",
      "710 [D loss: 13.616612] [G loss: 0.828765]\n",
      "711 [D loss: 14.896903] [G loss: 0.771190]\n",
      "712 [D loss: 14.747796] [G loss: 1.063481]\n",
      "713 [D loss: 15.187928] [G loss: 0.628940]\n",
      "714 [D loss: 15.508089] [G loss: 0.583515]\n",
      "715 [D loss: 14.948759] [G loss: 0.631549]\n",
      "716 [D loss: 14.689220] [G loss: 0.716090]\n",
      "717 [D loss: 14.415825] [G loss: 1.041212]\n",
      "718 [D loss: 16.653488] [G loss: 0.889560]\n",
      "719 [D loss: 15.204742] [G loss: 0.649766]\n",
      "720 [D loss: 14.129959] [G loss: 0.955692]\n",
      "721 [D loss: 15.389350] [G loss: 0.915210]\n",
      "722 [D loss: 13.632320] [G loss: 0.710007]\n",
      "723 [D loss: 14.487422] [G loss: 0.838245]\n",
      "724 [D loss: 14.421328] [G loss: 0.626065]\n",
      "725 [D loss: 13.602133] [G loss: 0.951470]\n",
      "726 [D loss: 13.543076] [G loss: 1.081099]\n",
      "727 [D loss: 14.947003] [G loss: 0.661787]\n",
      "728 [D loss: 15.475924] [G loss: 0.603272]\n",
      "729 [D loss: 13.773982] [G loss: 0.818340]\n",
      "./dataset_2018_05_16/12/\n",
      "730 [D loss: 14.335516] [G loss: 1.370933]\n",
      "731 [D loss: 13.508356] [G loss: 0.642247]\n",
      "732 [D loss: 15.285841] [G loss: 1.342313]\n",
      "733 [D loss: 13.940501] [G loss: 0.868872]\n",
      "734 [D loss: 13.907608] [G loss: 0.893641]\n",
      "735 [D loss: 14.147815] [G loss: 0.583957]\n",
      "736 [D loss: 13.572431] [G loss: 1.498749]\n",
      "737 [D loss: 11.893900] [G loss: 0.808603]\n",
      "738 [D loss: 14.264598] [G loss: 1.141343]\n",
      "739 [D loss: 12.960205] [G loss: 1.258961]\n",
      "740 [D loss: 14.542701] [G loss: 1.052217]\n",
      "741 [D loss: 14.142241] [G loss: 0.865317]\n",
      "742 [D loss: 15.297784] [G loss: 0.604703]\n",
      "743 [D loss: 16.150549] [G loss: 0.225465]\n",
      "744 [D loss: 14.698896] [G loss: 0.953562]\n",
      "745 [D loss: 15.199963] [G loss: 1.003577]\n",
      "746 [D loss: 14.955784] [G loss: 0.795272]\n",
      "747 [D loss: 14.599998] [G loss: 0.819908]\n",
      "748 [D loss: 13.918734] [G loss: 0.712443]\n",
      "749 [D loss: 14.243505] [G loss: 0.659112]\n",
      "750 [D loss: 14.411277] [G loss: 1.128080]\n",
      "751 [D loss: 15.068689] [G loss: 1.046907]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "752 [D loss: 14.795834] [G loss: 0.941530]\n",
      "753 [D loss: 13.850832] [G loss: 0.587970]\n",
      "754 [D loss: 14.889607] [G loss: 0.931540]\n",
      "755 [D loss: 14.656010] [G loss: 0.794870]\n",
      "756 [D loss: 14.300636] [G loss: 0.591965]\n",
      "757 [D loss: 15.040442] [G loss: 0.766560]\n",
      "./dataset_2018_05_16/13/\n",
      "758 [D loss: 15.565246] [G loss: 1.351806]\n",
      "759 [D loss: 14.320103] [G loss: 0.584231]\n",
      "760 [D loss: 13.644191] [G loss: 0.983964]\n",
      "761 [D loss: 14.327674] [G loss: 0.938419]\n",
      "762 [D loss: 12.464134] [G loss: 0.987418]\n",
      "763 [D loss: 14.911544] [G loss: 0.754603]\n",
      "764 [D loss: 14.593124] [G loss: 0.870909]\n",
      "765 [D loss: 14.170301] [G loss: 1.574885]\n",
      "766 [D loss: 15.659973] [G loss: 0.963788]\n",
      "767 [D loss: 13.776670] [G loss: 0.838141]\n",
      "768 [D loss: 13.297202] [G loss: 1.160670]\n",
      "769 [D loss: 16.570667] [G loss: 1.213026]\n",
      "770 [D loss: 13.815454] [G loss: 0.766746]\n",
      "771 [D loss: 14.056051] [G loss: 0.286219]\n",
      "772 [D loss: 14.652216] [G loss: 1.139117]\n",
      "773 [D loss: 13.559019] [G loss: 1.236506]\n",
      "774 [D loss: 14.405059] [G loss: 0.999555]\n",
      "775 [D loss: 13.639795] [G loss: 0.701641]\n",
      "776 [D loss: 13.564155] [G loss: 0.880204]\n",
      "777 [D loss: 14.376380] [G loss: 0.826993]\n",
      "778 [D loss: 12.801510] [G loss: 0.739102]\n",
      "779 [D loss: 13.217332] [G loss: 0.841774]\n",
      "780 [D loss: 13.908644] [G loss: 0.833385]\n",
      "781 [D loss: 15.533648] [G loss: 0.712661]\n",
      "782 [D loss: 14.191629] [G loss: 0.658778]\n",
      "783 [D loss: 14.417864] [G loss: 0.480924]\n",
      "784 [D loss: 14.697794] [G loss: 1.054968]\n",
      "785 [D loss: 14.364219] [G loss: 1.260878]\n",
      "./dataset_2018_05_16/14/\n",
      "786 [D loss: 13.964862] [G loss: 0.975207]\n",
      "787 [D loss: 12.947270] [G loss: 1.103983]\n",
      "788 [D loss: 14.219472] [G loss: 0.845182]\n",
      "789 [D loss: 13.930284] [G loss: 0.776082]\n",
      "790 [D loss: 15.026157] [G loss: 1.256330]\n",
      "791 [D loss: 13.690276] [G loss: 1.143159]\n",
      "792 [D loss: 15.144341] [G loss: 0.762132]\n",
      "793 [D loss: 14.149282] [G loss: 1.000669]\n",
      "794 [D loss: 13.477367] [G loss: 0.957653]\n",
      "795 [D loss: 13.881454] [G loss: 0.812195]\n",
      "796 [D loss: 15.139041] [G loss: 1.111963]\n",
      "797 [D loss: 14.053653] [G loss: 0.809228]\n",
      "798 [D loss: 15.087271] [G loss: 0.786661]\n",
      "799 [D loss: 15.972414] [G loss: 1.392491]\n",
      "800 [D loss: 13.566954] [G loss: 1.242785]\n",
      "801 [D loss: 14.158850] [G loss: 0.707807]\n",
      "802 [D loss: 15.058151] [G loss: 0.865833]\n",
      "803 [D loss: 13.904941] [G loss: 0.891509]\n",
      "804 [D loss: 14.884768] [G loss: 0.511985]\n",
      "805 [D loss: 12.518553] [G loss: 0.836487]\n",
      "806 [D loss: 14.422857] [G loss: 1.238559]\n",
      "807 [D loss: 13.629517] [G loss: 1.006059]\n",
      "808 [D loss: 14.270818] [G loss: 0.820624]\n",
      "809 [D loss: 15.478771] [G loss: 0.843790]\n",
      "810 [D loss: 13.690880] [G loss: 1.098032]\n",
      "811 [D loss: 15.503878] [G loss: 0.264920]\n",
      "812 [D loss: 15.143620] [G loss: 0.720082]\n",
      "813 [D loss: 14.501705] [G loss: 0.948853]\n",
      "./dataset_2018_05_16/15/\n",
      "814 [D loss: 15.221221] [G loss: 0.338444]\n",
      "815 [D loss: 13.647389] [G loss: 0.852078]\n",
      "816 [D loss: 13.573878] [G loss: 0.582987]\n",
      "817 [D loss: 14.011185] [G loss: 0.733037]\n",
      "818 [D loss: 15.295732] [G loss: 0.325388]\n",
      "819 [D loss: 15.416555] [G loss: 0.740601]\n",
      "820 [D loss: 14.634638] [G loss: 0.710853]\n",
      "821 [D loss: 15.502078] [G loss: 0.892834]\n",
      "822 [D loss: 12.780793] [G loss: 1.378732]\n",
      "823 [D loss: 15.869893] [G loss: 0.707406]\n",
      "824 [D loss: 14.558619] [G loss: 0.530249]\n",
      "825 [D loss: 14.891632] [G loss: 0.934727]\n",
      "826 [D loss: 16.597317] [G loss: 1.189603]\n",
      "827 [D loss: 14.473096] [G loss: 1.204023]\n",
      "828 [D loss: 13.704774] [G loss: 1.038933]\n",
      "829 [D loss: 14.828732] [G loss: 1.028673]\n",
      "830 [D loss: 13.969890] [G loss: 0.732942]\n",
      "831 [D loss: 15.237989] [G loss: 0.533115]\n",
      "832 [D loss: 15.043627] [G loss: 0.780136]\n",
      "833 [D loss: 13.692465] [G loss: 0.926132]\n",
      "834 [D loss: 14.822246] [G loss: 0.627902]\n",
      "835 [D loss: 14.676360] [G loss: 0.598606]\n",
      "836 [D loss: 14.879714] [G loss: 1.310238]\n",
      "837 [D loss: 14.221151] [G loss: 0.811417]\n",
      "838 [D loss: 13.109344] [G loss: 1.036475]\n",
      "839 [D loss: 15.274655] [G loss: 0.567123]\n",
      "840 [D loss: 15.925837] [G loss: 0.774415]\n",
      "841 [D loss: 13.741763] [G loss: 1.152248]\n",
      "842 [D loss: 15.180423] [G loss: 1.127785]\n",
      "./dataset_2018_05_16/1/\n",
      "843 [D loss: 14.170568] [G loss: 0.588453]\n",
      "844 [D loss: 13.959100] [G loss: 0.756289]\n",
      "845 [D loss: 15.766177] [G loss: 0.514439]\n",
      "846 [D loss: 14.976670] [G loss: 1.117688]\n",
      "847 [D loss: 14.309902] [G loss: 0.660328]\n",
      "848 [D loss: 14.147870] [G loss: 1.022632]\n",
      "849 [D loss: 15.042856] [G loss: 0.463558]\n",
      "850 [D loss: 14.502457] [G loss: 0.527651]\n",
      "851 [D loss: 13.705925] [G loss: 0.805839]\n",
      "852 [D loss: 15.053457] [G loss: 1.381823]\n",
      "853 [D loss: 13.261415] [G loss: 1.066000]\n",
      "854 [D loss: 14.114622] [G loss: 0.923813]\n",
      "855 [D loss: 13.418827] [G loss: 1.064579]\n",
      "856 [D loss: 15.401940] [G loss: 0.596346]\n",
      "857 [D loss: 14.826011] [G loss: 0.582261]\n",
      "858 [D loss: 13.938454] [G loss: 0.228548]\n",
      "859 [D loss: 15.415346] [G loss: 1.052615]\n",
      "860 [D loss: 13.631121] [G loss: 1.334286]\n",
      "861 [D loss: 14.388413] [G loss: 1.279769]\n",
      "862 [D loss: 15.882469] [G loss: 1.156375]\n",
      "863 [D loss: 13.087109] [G loss: 1.218990]\n",
      "864 [D loss: 15.609554] [G loss: 0.942906]\n",
      "865 [D loss: 15.078224] [G loss: 0.588675]\n",
      "866 [D loss: 13.988750] [G loss: 1.195872]\n",
      "867 [D loss: 14.494261] [G loss: 0.526182]\n",
      "868 [D loss: 13.031484] [G loss: 1.491304]\n",
      "869 [D loss: 14.865863] [G loss: 0.893092]\n",
      "./dataset_2018_05_16/2/\n",
      "870 [D loss: 14.597567] [G loss: 0.564131]\n",
      "871 [D loss: 14.386574] [G loss: 1.054511]\n",
      "872 [D loss: 14.680708] [G loss: 0.818328]\n",
      "873 [D loss: 13.915621] [G loss: 0.830209]\n",
      "874 [D loss: 14.209294] [G loss: 0.457378]\n",
      "875 [D loss: 14.348232] [G loss: 0.868532]\n",
      "876 [D loss: 15.444766] [G loss: 0.944506]\n",
      "877 [D loss: 13.477278] [G loss: 0.465442]\n",
      "878 [D loss: 15.785023] [G loss: 0.682584]\n",
      "879 [D loss: 15.018223] [G loss: 1.056738]\n",
      "880 [D loss: 14.552356] [G loss: 0.717163]\n",
      "881 [D loss: 15.109305] [G loss: 0.634223]\n",
      "882 [D loss: 14.625959] [G loss: 0.628245]\n",
      "883 [D loss: 14.219134] [G loss: 0.801898]\n",
      "884 [D loss: 14.779262] [G loss: 0.515864]\n",
      "885 [D loss: 14.053801] [G loss: 0.717185]\n",
      "886 [D loss: 14.249761] [G loss: 0.049676]\n",
      "887 [D loss: 14.378036] [G loss: 0.816524]\n",
      "888 [D loss: 14.271518] [G loss: 1.534844]\n",
      "889 [D loss: 13.734320] [G loss: 1.108146]\n",
      "890 [D loss: 12.989752] [G loss: 0.392894]\n",
      "891 [D loss: 14.279238] [G loss: 0.534524]\n",
      "892 [D loss: 15.085962] [G loss: 0.923657]\n",
      "893 [D loss: 15.998650] [G loss: 1.305624]\n",
      "894 [D loss: 14.225497] [G loss: 1.103118]\n",
      "895 [D loss: 14.614431] [G loss: 1.148532]\n",
      "896 [D loss: 14.371908] [G loss: 0.009924]\n",
      "897 [D loss: 14.934761] [G loss: 0.796818]\n",
      "898 [D loss: 14.165005] [G loss: 0.852667]\n",
      "./dataset_2018_05_16/3/\n",
      "899 [D loss: 14.457609] [G loss: 1.039707]\n",
      "900 [D loss: 14.389874] [G loss: 0.456857]\n",
      "901 [D loss: 13.626390] [G loss: 0.357764]\n",
      "902 [D loss: 13.524590] [G loss: 0.316893]\n",
      "903 [D loss: 14.955619] [G loss: 1.338179]\n",
      "904 [D loss: 13.852284] [G loss: 0.864713]\n",
      "905 [D loss: 14.104849] [G loss: 1.073682]\n",
      "906 [D loss: 14.487691] [G loss: 0.949200]\n",
      "907 [D loss: 13.679088] [G loss: 0.535093]\n",
      "908 [D loss: 16.285322] [G loss: 0.727357]\n",
      "909 [D loss: 13.532260] [G loss: 0.916688]\n",
      "910 [D loss: 14.248465] [G loss: 0.687305]\n",
      "911 [D loss: 13.723579] [G loss: 0.569100]\n",
      "912 [D loss: 13.669047] [G loss: 0.495618]\n",
      "913 [D loss: 16.955645] [G loss: 0.742019]\n",
      "914 [D loss: 12.744235] [G loss: 1.283259]\n",
      "915 [D loss: 14.022906] [G loss: 0.553418]\n",
      "916 [D loss: 13.424284] [G loss: 0.944615]\n",
      "917 [D loss: 13.620863] [G loss: 0.522350]\n",
      "918 [D loss: 14.425600] [G loss: 1.066990]\n",
      "919 [D loss: 13.321765] [G loss: 0.863427]\n",
      "920 [D loss: 14.510483] [G loss: 0.787789]\n",
      "921 [D loss: 13.514549] [G loss: 0.370848]\n",
      "922 [D loss: 14.209417] [G loss: 0.729558]\n",
      "923 [D loss: 14.275777] [G loss: 0.466330]\n",
      "924 [D loss: 15.006413] [G loss: 0.997108]\n",
      "925 [D loss: 15.064719] [G loss: 0.683678]\n",
      "926 [D loss: 13.955427] [G loss: 1.066143]\n",
      "./dataset_2018_05_16/4/\n",
      "927 [D loss: 16.375565] [G loss: 1.330011]\n",
      "928 [D loss: 13.253111] [G loss: 0.350724]\n",
      "929 [D loss: 15.344969] [G loss: 0.812259]\n",
      "930 [D loss: 14.848990] [G loss: 1.101296]\n",
      "931 [D loss: 13.569506] [G loss: 0.486182]\n",
      "932 [D loss: 15.142095] [G loss: 0.470331]\n",
      "933 [D loss: 14.538377] [G loss: 0.789830]\n",
      "934 [D loss: 13.898034] [G loss: 0.577317]\n",
      "935 [D loss: 14.047731] [G loss: 0.913288]\n",
      "936 [D loss: 13.666789] [G loss: 1.099003]\n",
      "937 [D loss: 14.353642] [G loss: 0.767958]\n",
      "938 [D loss: 16.162992] [G loss: 0.571749]\n",
      "939 [D loss: 13.912918] [G loss: 0.738244]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "940 [D loss: 14.051367] [G loss: 0.902074]\n",
      "941 [D loss: 14.201212] [G loss: 0.538392]\n",
      "942 [D loss: 13.689951] [G loss: 0.986566]\n",
      "943 [D loss: 14.459675] [G loss: 0.491899]\n",
      "944 [D loss: 14.522274] [G loss: 0.992477]\n",
      "945 [D loss: 14.899453] [G loss: 0.370897]\n",
      "946 [D loss: 14.799355] [G loss: 0.873489]\n",
      "947 [D loss: 13.969109] [G loss: 0.874276]\n",
      "948 [D loss: 14.902170] [G loss: 1.145006]\n",
      "949 [D loss: 11.836753] [G loss: 0.871283]\n",
      "950 [D loss: 15.401567] [G loss: 0.747339]\n",
      "951 [D loss: 12.235869] [G loss: 0.928475]\n",
      "952 [D loss: 14.780125] [G loss: 1.155107]\n",
      "953 [D loss: 14.492407] [G loss: 0.569915]\n",
      "954 [D loss: 13.006274] [G loss: 0.577412]\n",
      "./dataset_2018_05_16/5/\n",
      "955 [D loss: 15.439568] [G loss: 1.402355]\n",
      "956 [D loss: 11.844077] [G loss: 0.879253]\n",
      "957 [D loss: 14.016349] [G loss: 0.859386]\n",
      "958 [D loss: 14.335340] [G loss: 0.743413]\n",
      "959 [D loss: 13.852217] [G loss: 0.724884]\n",
      "960 [D loss: 15.255304] [G loss: 0.437181]\n",
      "961 [D loss: 14.459753] [G loss: 1.039362]\n",
      "962 [D loss: 12.954836] [G loss: 0.483877]\n",
      "963 [D loss: 13.505605] [G loss: 0.870466]\n",
      "964 [D loss: 15.592050] [G loss: 0.672278]\n",
      "965 [D loss: 14.354324] [G loss: 1.158847]\n",
      "966 [D loss: 14.770382] [G loss: 1.100649]\n",
      "967 [D loss: 14.711155] [G loss: 1.330048]\n",
      "968 [D loss: 13.071950] [G loss: 1.086058]\n",
      "969 [D loss: 14.039054] [G loss: 0.631038]\n",
      "970 [D loss: 14.335361] [G loss: 0.650895]\n",
      "971 [D loss: 13.995969] [G loss: 0.802338]\n",
      "972 [D loss: 14.526041] [G loss: 0.569714]\n",
      "973 [D loss: 14.602359] [G loss: 0.603263]\n",
      "974 [D loss: 13.179794] [G loss: 0.874006]\n",
      "975 [D loss: 13.094846] [G loss: 0.319286]\n",
      "976 [D loss: 15.063210] [G loss: 0.174702]\n",
      "977 [D loss: 14.757840] [G loss: 0.574778]\n",
      "978 [D loss: 13.621637] [G loss: 0.474288]\n",
      "979 [D loss: 14.268482] [G loss: 1.141511]\n",
      "980 [D loss: 13.918201] [G loss: 1.028475]\n",
      "981 [D loss: 13.802856] [G loss: 0.588391]\n",
      "982 [D loss: 13.823576] [G loss: 0.849225]\n",
      "./dataset_2018_05_16/6/\n",
      "983 [D loss: 13.890335] [G loss: 0.323665]\n",
      "984 [D loss: 14.602926] [G loss: -0.074624]\n",
      "985 [D loss: 14.125444] [G loss: 0.605348]\n",
      "986 [D loss: 13.615503] [G loss: 1.128880]\n",
      "987 [D loss: 14.393785] [G loss: 0.959732]\n",
      "988 [D loss: 14.481102] [G loss: 0.413333]\n",
      "989 [D loss: 14.741632] [G loss: 0.766559]\n",
      "990 [D loss: 15.330187] [G loss: 0.343890]\n",
      "991 [D loss: 14.880644] [G loss: 0.707912]\n",
      "992 [D loss: 15.198645] [G loss: 0.753710]\n",
      "993 [D loss: 12.403207] [G loss: 0.190446]\n",
      "994 [D loss: 13.340334] [G loss: 0.611537]\n",
      "995 [D loss: 13.535488] [G loss: 1.047620]\n",
      "996 [D loss: 14.399164] [G loss: 0.534456]\n",
      "997 [D loss: 13.434691] [G loss: 0.570652]\n",
      "998 [D loss: 14.854679] [G loss: 1.006027]\n",
      "999 [D loss: 13.866940] [G loss: 0.878249]\n",
      "1000 [D loss: 13.133281] [G loss: 0.311052]\n",
      "gan imaga2 :  (128, 128, 1)\n",
      "1001 [D loss: 14.861706] [G loss: 0.586741]\n",
      "1002 [D loss: 12.777837] [G loss: 1.210267]\n",
      "1003 [D loss: 14.921676] [G loss: 0.469007]\n",
      "1004 [D loss: 14.761890] [G loss: 0.783717]\n",
      "1005 [D loss: 14.835371] [G loss: 0.399863]\n",
      "1006 [D loss: 15.112835] [G loss: 0.929239]\n",
      "1007 [D loss: 13.506255] [G loss: 0.793094]\n",
      "1008 [D loss: 15.569212] [G loss: 0.108164]\n",
      "1009 [D loss: 15.750435] [G loss: 0.278531]\n",
      "1010 [D loss: 14.000594] [G loss: 1.135042]\n",
      "./dataset_2018_05_16/7/\n",
      "1011 [D loss: 15.673212] [G loss: 0.470869]\n",
      "1012 [D loss: 13.387876] [G loss: 0.803620]\n",
      "1013 [D loss: 12.778428] [G loss: 0.567960]\n",
      "1014 [D loss: 12.463214] [G loss: 1.034727]\n",
      "1015 [D loss: 16.304707] [G loss: 0.829664]\n",
      "1016 [D loss: 14.655888] [G loss: 0.754819]\n",
      "1017 [D loss: 15.744396] [G loss: 0.727906]\n",
      "1018 [D loss: 14.936491] [G loss: 0.951054]\n",
      "1019 [D loss: 13.845800] [G loss: 0.702435]\n",
      "1020 [D loss: 13.003032] [G loss: 0.685889]\n",
      "1021 [D loss: 15.513168] [G loss: 1.373608]\n",
      "1022 [D loss: 14.464041] [G loss: 1.159330]\n",
      "1023 [D loss: 15.337567] [G loss: 0.930432]\n",
      "1024 [D loss: 14.347421] [G loss: 0.548738]\n",
      "1025 [D loss: 14.545849] [G loss: 0.413321]\n",
      "1026 [D loss: 13.569749] [G loss: 0.729541]\n",
      "1027 [D loss: 14.330803] [G loss: 1.108968]\n",
      "1028 [D loss: 14.322271] [G loss: 0.819937]\n",
      "1029 [D loss: 14.722039] [G loss: 0.610059]\n",
      "1030 [D loss: 14.648859] [G loss: 0.962725]\n",
      "1031 [D loss: 13.478147] [G loss: 0.750625]\n",
      "1032 [D loss: 13.843720] [G loss: 0.517897]\n",
      "1033 [D loss: 14.693474] [G loss: 0.570320]\n",
      "1034 [D loss: 13.959495] [G loss: 0.206186]\n",
      "1035 [D loss: 15.432728] [G loss: 1.055422]\n",
      "1036 [D loss: 14.967081] [G loss: 0.427232]\n",
      "1037 [D loss: 13.955030] [G loss: 0.945953]\n",
      "1038 [D loss: 12.881993] [G loss: 0.838479]\n",
      "./dataset_2018_05_16/8/\n",
      "1039 [D loss: 14.184828] [G loss: 0.934943]\n",
      "1040 [D loss: 15.207040] [G loss: 0.492069]\n",
      "1041 [D loss: 14.841830] [G loss: 0.813128]\n",
      "1042 [D loss: 13.675803] [G loss: 0.352280]\n",
      "1043 [D loss: 14.507864] [G loss: 0.744512]\n",
      "1044 [D loss: 12.858468] [G loss: 0.903921]\n",
      "1045 [D loss: 14.468803] [G loss: 0.873381]\n",
      "1046 [D loss: 13.250965] [G loss: 0.701038]\n",
      "1047 [D loss: 15.008241] [G loss: 1.244150]\n",
      "1048 [D loss: 13.094184] [G loss: 1.153966]\n",
      "1049 [D loss: 14.062181] [G loss: 0.657325]\n",
      "1050 [D loss: 13.564353] [G loss: 0.967183]\n",
      "1051 [D loss: 13.491519] [G loss: 0.960510]\n",
      "1052 [D loss: 15.289454] [G loss: 1.141035]\n",
      "1053 [D loss: 13.407728] [G loss: 1.104523]\n",
      "1054 [D loss: 14.768167] [G loss: 0.921317]\n",
      "1055 [D loss: 14.754927] [G loss: 0.933570]\n",
      "1056 [D loss: 14.698917] [G loss: 0.849962]\n",
      "1057 [D loss: 14.032233] [G loss: 0.834599]\n",
      "1058 [D loss: 14.898646] [G loss: 0.400589]\n",
      "1059 [D loss: 14.373712] [G loss: 1.118589]\n",
      "1060 [D loss: 12.957204] [G loss: 0.823336]\n",
      "1061 [D loss: 14.946658] [G loss: 1.042217]\n",
      "1062 [D loss: 14.239714] [G loss: 0.963912]\n",
      "1063 [D loss: 13.445345] [G loss: 0.474518]\n",
      "1064 [D loss: 14.678353] [G loss: 0.774037]\n",
      "1065 [D loss: 13.982822] [G loss: 0.682707]\n",
      "1066 [D loss: 15.546371] [G loss: 1.046106]\n",
      "./dataset_2018_05_16/9/\n",
      "1067 [D loss: 14.084235] [G loss: 0.502827]\n",
      "1068 [D loss: 14.472746] [G loss: 0.745818]\n",
      "1069 [D loss: 15.212984] [G loss: 0.772350]\n",
      "1070 [D loss: 14.230775] [G loss: 0.837231]\n",
      "1071 [D loss: 14.608219] [G loss: 0.500712]\n",
      "1072 [D loss: 13.777709] [G loss: 0.738025]\n",
      "1073 [D loss: 13.897782] [G loss: 0.698925]\n",
      "1074 [D loss: 14.316218] [G loss: 0.508363]\n",
      "1075 [D loss: 13.407957] [G loss: 1.027771]\n",
      "1076 [D loss: 15.039005] [G loss: 0.744006]\n",
      "1077 [D loss: 13.925107] [G loss: 1.035754]\n",
      "1078 [D loss: 14.596016] [G loss: 0.868625]\n",
      "1079 [D loss: 13.912637] [G loss: 0.458062]\n",
      "1080 [D loss: 14.691533] [G loss: 1.246135]\n",
      "1081 [D loss: 13.518663] [G loss: 1.152500]\n",
      "1082 [D loss: 14.729505] [G loss: 1.138620]\n",
      "1083 [D loss: 15.046493] [G loss: 0.657447]\n",
      "1084 [D loss: 15.146818] [G loss: 1.156843]\n",
      "1085 [D loss: 14.016273] [G loss: 1.051437]\n",
      "1086 [D loss: 13.736907] [G loss: 0.809883]\n",
      "1087 [D loss: 14.049474] [G loss: 1.238569]\n",
      "1088 [D loss: 14.517889] [G loss: 0.517771]\n",
      "1089 [D loss: 13.954081] [G loss: 1.052641]\n",
      "1090 [D loss: 14.706885] [G loss: 1.184477]\n",
      "1091 [D loss: 13.406576] [G loss: 1.182544]\n",
      "1092 [D loss: 15.225480] [G loss: 1.277179]\n",
      "1093 [D loss: 13.739742] [G loss: 1.135936]\n",
      "1094 [D loss: 14.493994] [G loss: 1.294158]\n",
      "./dataset_2018_05_16/10/\n",
      "1095 [D loss: 14.699609] [G loss: 0.841472]\n",
      "1096 [D loss: 12.610156] [G loss: 0.964224]\n",
      "1097 [D loss: 15.024197] [G loss: 1.053756]\n",
      "1098 [D loss: 13.907045] [G loss: 1.025358]\n",
      "1099 [D loss: 16.327097] [G loss: 0.504916]\n",
      "1100 [D loss: 15.035402] [G loss: 0.592888]\n",
      "1101 [D loss: 13.578631] [G loss: 0.823610]\n",
      "1102 [D loss: 13.932529] [G loss: 0.890071]\n",
      "1103 [D loss: 15.163973] [G loss: 0.760871]\n",
      "1104 [D loss: 15.679857] [G loss: 0.783821]\n",
      "1105 [D loss: 15.354964] [G loss: 0.875896]\n",
      "1106 [D loss: 13.703982] [G loss: 1.139531]\n",
      "1107 [D loss: 14.445835] [G loss: 0.991935]\n",
      "1108 [D loss: 14.669840] [G loss: 1.116533]\n",
      "1109 [D loss: 13.365759] [G loss: 0.574739]\n",
      "1110 [D loss: 14.272430] [G loss: 0.547893]\n",
      "1111 [D loss: 15.658638] [G loss: 0.571359]\n",
      "1112 [D loss: 14.034029] [G loss: 0.864399]\n",
      "1113 [D loss: 13.340676] [G loss: 1.109937]\n",
      "1114 [D loss: 15.369892] [G loss: 0.500419]\n",
      "1115 [D loss: 14.281235] [G loss: 1.045309]\n",
      "1116 [D loss: 13.872143] [G loss: 0.734805]\n",
      "1117 [D loss: 13.828761] [G loss: 0.751400]\n",
      "1118 [D loss: 13.639277] [G loss: 0.789982]\n",
      "1119 [D loss: 14.877720] [G loss: 1.050792]\n",
      "1120 [D loss: 14.101036] [G loss: 0.646941]\n",
      "1121 [D loss: 15.381422] [G loss: 0.751204]\n",
      "1122 [D loss: 14.801138] [G loss: 0.731680]\n",
      "1123 [D loss: 14.960599] [G loss: 0.951629]\n",
      "./dataset_2018_05_16/11/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1124 [D loss: 13.859226] [G loss: 0.468996]\n",
      "1125 [D loss: 16.124920] [G loss: 1.178203]\n",
      "1126 [D loss: 14.775661] [G loss: 0.759353]\n",
      "1127 [D loss: 14.110192] [G loss: 1.021067]\n",
      "1128 [D loss: 14.173207] [G loss: 0.712400]\n",
      "1129 [D loss: 14.542517] [G loss: 0.574353]\n",
      "1130 [D loss: 15.020832] [G loss: 1.084302]\n",
      "1131 [D loss: 14.114505] [G loss: 0.716575]\n",
      "1132 [D loss: 12.901185] [G loss: 1.298359]\n",
      "1133 [D loss: 13.648646] [G loss: 0.978499]\n",
      "1134 [D loss: 14.144199] [G loss: 0.535538]\n",
      "1135 [D loss: 14.605224] [G loss: 0.725251]\n",
      "1136 [D loss: 14.756900] [G loss: 0.642934]\n",
      "1137 [D loss: 13.376423] [G loss: 0.547259]\n",
      "1138 [D loss: 14.209604] [G loss: 0.930685]\n",
      "1139 [D loss: 13.852098] [G loss: 1.044466]\n",
      "1140 [D loss: 13.989395] [G loss: 1.110661]\n",
      "1141 [D loss: 13.424279] [G loss: 0.739418]\n",
      "1142 [D loss: 13.258265] [G loss: 1.590565]\n",
      "1143 [D loss: 15.671098] [G loss: 0.562435]\n",
      "1144 [D loss: 16.416231] [G loss: 0.938915]\n",
      "1145 [D loss: 13.847425] [G loss: 0.768272]\n",
      "1146 [D loss: 14.768942] [G loss: 1.217902]\n",
      "1147 [D loss: 14.713753] [G loss: 0.754542]\n",
      "1148 [D loss: 14.172631] [G loss: 1.108953]\n",
      "1149 [D loss: 12.922271] [G loss: 0.178012]\n",
      "1150 [D loss: 15.652796] [G loss: 0.903256]\n",
      "1151 [D loss: 13.490033] [G loss: 0.800154]\n",
      "./dataset_2018_05_16/12/\n",
      "1152 [D loss: 13.308686] [G loss: 0.589535]\n",
      "1153 [D loss: 14.590250] [G loss: 0.553766]\n",
      "1154 [D loss: 14.807153] [G loss: 0.306868]\n",
      "1155 [D loss: 13.986219] [G loss: 0.847181]\n",
      "1156 [D loss: 14.403811] [G loss: 0.423956]\n",
      "1157 [D loss: 14.005540] [G loss: 0.776536]\n",
      "1158 [D loss: 15.298845] [G loss: 1.136172]\n",
      "1159 [D loss: 13.342719] [G loss: 0.682385]\n",
      "1160 [D loss: 14.258771] [G loss: 1.061651]\n",
      "1161 [D loss: 14.773155] [G loss: 1.040587]\n",
      "1162 [D loss: 13.840873] [G loss: 1.513670]\n",
      "1163 [D loss: 13.485004] [G loss: 0.398013]\n",
      "1164 [D loss: 14.225025] [G loss: 0.528730]\n",
      "1165 [D loss: 13.752518] [G loss: 1.311180]\n",
      "1166 [D loss: 13.952570] [G loss: 1.178845]\n",
      "1167 [D loss: 14.228352] [G loss: 1.396367]\n",
      "1168 [D loss: 13.602874] [G loss: 0.792957]\n",
      "1169 [D loss: 14.466570] [G loss: 0.920662]\n",
      "1170 [D loss: 14.313610] [G loss: 0.891175]\n",
      "1171 [D loss: 16.514021] [G loss: 1.191882]\n",
      "1172 [D loss: 14.421078] [G loss: 0.796808]\n",
      "1173 [D loss: 14.728031] [G loss: 0.716080]\n",
      "1174 [D loss: 14.332422] [G loss: 0.689104]\n",
      "1175 [D loss: 12.978275] [G loss: 0.748515]\n",
      "1176 [D loss: 14.337079] [G loss: 0.661725]\n",
      "1177 [D loss: 13.996789] [G loss: 0.473336]\n",
      "1178 [D loss: 14.642545] [G loss: 0.871750]\n",
      "1179 [D loss: 14.294120] [G loss: 0.917116]\n",
      "./dataset_2018_05_16/13/\n",
      "1180 [D loss: 14.964767] [G loss: 0.531931]\n",
      "1181 [D loss: 14.530197] [G loss: 0.937122]\n",
      "1182 [D loss: 14.159035] [G loss: 0.996395]\n",
      "1183 [D loss: 14.843749] [G loss: 0.815539]\n",
      "1184 [D loss: 14.348900] [G loss: 0.819426]\n",
      "1185 [D loss: 15.034358] [G loss: 0.630160]\n",
      "1186 [D loss: 13.929014] [G loss: 1.112864]\n",
      "1187 [D loss: 13.980539] [G loss: 0.703799]\n",
      "1188 [D loss: 13.478450] [G loss: 0.587485]\n",
      "1189 [D loss: 14.353003] [G loss: 0.967912]\n",
      "1190 [D loss: 13.464389] [G loss: 0.498389]\n",
      "1191 [D loss: 14.297705] [G loss: 1.510109]\n",
      "1192 [D loss: 14.467304] [G loss: 0.824972]\n",
      "1193 [D loss: 15.102874] [G loss: 0.656102]\n",
      "1194 [D loss: 15.249488] [G loss: 0.856904]\n",
      "1195 [D loss: 14.091465] [G loss: 0.636353]\n",
      "1196 [D loss: 13.816453] [G loss: 1.257957]\n",
      "1197 [D loss: 14.794188] [G loss: 0.674921]\n",
      "1198 [D loss: 15.885799] [G loss: 0.672196]\n",
      "1199 [D loss: 12.005581] [G loss: 0.285276]\n",
      "1200 [D loss: 14.957930] [G loss: 1.142565]\n",
      "1201 [D loss: 15.396250] [G loss: 1.442184]\n",
      "1202 [D loss: 14.412192] [G loss: 0.751170]\n",
      "1203 [D loss: 13.878584] [G loss: 0.992866]\n",
      "1204 [D loss: 12.749353] [G loss: 0.753845]\n",
      "1205 [D loss: 13.472509] [G loss: 0.534574]\n",
      "1206 [D loss: 15.839083] [G loss: 1.448748]\n",
      "1207 [D loss: 14.328746] [G loss: 1.077456]\n",
      "./dataset_2018_05_16/14/\n",
      "1208 [D loss: 13.752394] [G loss: 0.836421]\n",
      "1209 [D loss: 14.213990] [G loss: 0.697366]\n",
      "1210 [D loss: 15.981821] [G loss: 0.707509]\n",
      "1211 [D loss: 14.350794] [G loss: 1.259987]\n",
      "1212 [D loss: 12.726625] [G loss: 1.250144]\n",
      "1213 [D loss: 14.711679] [G loss: 1.137971]\n",
      "1214 [D loss: 14.979735] [G loss: 0.961410]\n",
      "1215 [D loss: 14.135166] [G loss: 1.018385]\n",
      "1216 [D loss: 14.339294] [G loss: 0.891185]\n",
      "1217 [D loss: 12.008766] [G loss: 0.383295]\n",
      "1218 [D loss: 14.347049] [G loss: 0.599930]\n",
      "1219 [D loss: 14.267504] [G loss: 1.062117]\n",
      "1220 [D loss: 16.159992] [G loss: 1.054695]\n",
      "1221 [D loss: 14.602593] [G loss: 1.240611]\n",
      "1222 [D loss: 14.351121] [G loss: 1.146613]\n",
      "1223 [D loss: 14.380381] [G loss: 0.782253]\n",
      "1224 [D loss: 12.943515] [G loss: 0.672536]\n",
      "1225 [D loss: 15.006838] [G loss: 0.528905]\n",
      "1226 [D loss: 14.839856] [G loss: 0.677212]\n",
      "1227 [D loss: 14.910042] [G loss: 0.772927]\n",
      "1228 [D loss: 13.939535] [G loss: 0.987055]\n",
      "1229 [D loss: 13.144587] [G loss: 0.720023]\n",
      "1230 [D loss: 15.113461] [G loss: 0.659248]\n",
      "1231 [D loss: 13.758345] [G loss: 0.728235]\n",
      "1232 [D loss: 13.717986] [G loss: 0.820641]\n",
      "1233 [D loss: 12.475938] [G loss: 0.653485]\n",
      "1234 [D loss: 13.491178] [G loss: 0.831573]\n",
      "1235 [D loss: 14.311926] [G loss: 0.840030]\n",
      "./dataset_2018_05_16/15/\n",
      "1236 [D loss: 14.462912] [G loss: 1.190506]\n",
      "1237 [D loss: 14.188211] [G loss: 0.732079]\n",
      "1238 [D loss: 13.363840] [G loss: 1.200808]\n",
      "1239 [D loss: 15.096508] [G loss: 0.787751]\n",
      "1240 [D loss: 13.687140] [G loss: 0.451428]\n",
      "1241 [D loss: 15.183474] [G loss: 1.453866]\n",
      "1242 [D loss: 16.457474] [G loss: 0.805138]\n",
      "1243 [D loss: 14.227078] [G loss: 1.028058]\n",
      "1244 [D loss: 14.819234] [G loss: 0.512144]\n",
      "1245 [D loss: 13.883078] [G loss: 1.278327]\n",
      "1246 [D loss: 14.194443] [G loss: 0.694442]\n",
      "1247 [D loss: 14.581520] [G loss: 0.647233]\n",
      "1248 [D loss: 13.740431] [G loss: 0.923386]\n",
      "1249 [D loss: 13.554893] [G loss: 0.723566]\n",
      "1250 [D loss: 13.488825] [G loss: 0.651497]\n",
      "1251 [D loss: 13.625867] [G loss: 1.104633]\n",
      "1252 [D loss: 14.770182] [G loss: 0.640017]\n",
      "1253 [D loss: 13.198196] [G loss: 0.912459]\n",
      "1254 [D loss: 14.050855] [G loss: 0.478817]\n",
      "1255 [D loss: 15.214427] [G loss: 1.468800]\n",
      "1256 [D loss: 13.440581] [G loss: 0.847575]\n",
      "1257 [D loss: 15.344953] [G loss: 0.373474]\n",
      "1258 [D loss: 14.408120] [G loss: 0.901813]\n",
      "1259 [D loss: 15.454004] [G loss: 0.980573]\n",
      "1260 [D loss: 15.060823] [G loss: 0.681298]\n",
      "1261 [D loss: 14.182126] [G loss: 0.717672]\n",
      "1262 [D loss: 13.462130] [G loss: 1.083517]\n",
      "1263 [D loss: 15.429632] [G loss: 0.619282]\n",
      "./dataset_2018_05_16/1/\n",
      "1264 [D loss: 13.447039] [G loss: 0.389460]\n",
      "1265 [D loss: 12.921476] [G loss: 0.767274]\n",
      "1266 [D loss: 14.488536] [G loss: 0.972355]\n",
      "1267 [D loss: 13.811399] [G loss: 0.405726]\n",
      "1268 [D loss: 14.157054] [G loss: 1.094741]\n",
      "1269 [D loss: 13.386869] [G loss: 0.816386]\n",
      "1270 [D loss: 14.196321] [G loss: 0.922757]\n",
      "1271 [D loss: 14.732069] [G loss: 0.554324]\n",
      "1272 [D loss: 12.539103] [G loss: 0.513094]\n",
      "1273 [D loss: 13.931578] [G loss: 1.346309]\n",
      "1274 [D loss: 13.423554] [G loss: 0.378631]\n",
      "1275 [D loss: 14.654618] [G loss: 0.507566]\n",
      "1276 [D loss: 14.775492] [G loss: 1.187730]\n",
      "1277 [D loss: 13.967681] [G loss: 0.901711]\n",
      "1278 [D loss: 14.150186] [G loss: 1.111498]\n",
      "1279 [D loss: 14.244177] [G loss: 1.190453]\n",
      "1280 [D loss: 14.710127] [G loss: 1.276852]\n",
      "1281 [D loss: 12.974253] [G loss: 0.907512]\n",
      "1282 [D loss: 13.668457] [G loss: 0.850740]\n",
      "1283 [D loss: 14.450202] [G loss: 0.694031]\n",
      "1284 [D loss: 14.077183] [G loss: 0.618468]\n",
      "1285 [D loss: 14.449091] [G loss: 0.903246]\n",
      "1286 [D loss: 14.134495] [G loss: 1.075253]\n",
      "1287 [D loss: 13.972370] [G loss: 1.139650]\n",
      "1288 [D loss: 15.335054] [G loss: 0.261305]\n",
      "1289 [D loss: 13.367551] [G loss: 0.542493]\n",
      "1290 [D loss: 13.647972] [G loss: 0.186102]\n",
      "1291 [D loss: 15.100719] [G loss: 1.310042]\n",
      "./dataset_2018_05_16/2/\n",
      "1292 [D loss: 13.408615] [G loss: 0.637752]\n",
      "1293 [D loss: 12.458754] [G loss: 0.993974]\n",
      "1294 [D loss: 13.677124] [G loss: 1.037822]\n",
      "1295 [D loss: 15.784343] [G loss: 0.871820]\n",
      "1296 [D loss: 13.867503] [G loss: 0.941293]\n",
      "1297 [D loss: 14.153472] [G loss: 0.870986]\n",
      "1298 [D loss: 13.148075] [G loss: 1.037854]\n",
      "1299 [D loss: 14.450222] [G loss: 0.628188]\n",
      "1300 [D loss: 15.103928] [G loss: 1.038193]\n",
      "1301 [D loss: 14.949357] [G loss: 1.063284]\n",
      "1302 [D loss: 15.085777] [G loss: 0.536247]\n",
      "1303 [D loss: 13.640499] [G loss: 0.695706]\n",
      "1304 [D loss: 13.494517] [G loss: 0.746944]\n",
      "1305 [D loss: 14.907281] [G loss: 1.272040]\n",
      "1306 [D loss: 14.879179] [G loss: 0.338390]\n",
      "1307 [D loss: 13.576209] [G loss: 0.706650]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1308 [D loss: 15.050111] [G loss: 0.629135]\n",
      "1309 [D loss: 13.669785] [G loss: 0.606735]\n",
      "1310 [D loss: 15.151925] [G loss: 0.912833]\n",
      "1311 [D loss: 15.508574] [G loss: 0.721542]\n",
      "1312 [D loss: 14.432536] [G loss: 0.567471]\n",
      "1313 [D loss: 15.718801] [G loss: 1.334877]\n",
      "1314 [D loss: 14.710875] [G loss: 1.088668]\n",
      "1315 [D loss: 12.645543] [G loss: 0.490857]\n",
      "1316 [D loss: 13.938638] [G loss: 0.887567]\n",
      "1317 [D loss: 14.690598] [G loss: 1.070586]\n",
      "1318 [D loss: 14.081528] [G loss: 0.845867]\n",
      "1319 [D loss: 13.574107] [G loss: 0.327829]\n",
      "./dataset_2018_05_16/3/\n",
      "1320 [D loss: 15.493622] [G loss: 0.790289]\n",
      "1321 [D loss: 14.944880] [G loss: 1.338596]\n",
      "1322 [D loss: 15.596830] [G loss: 0.847537]\n",
      "1323 [D loss: 13.011415] [G loss: 0.129156]\n",
      "1324 [D loss: 13.418220] [G loss: 1.025440]\n",
      "1325 [D loss: 15.729980] [G loss: 0.635713]\n",
      "1326 [D loss: 12.797628] [G loss: 0.789555]\n",
      "1327 [D loss: 12.334536] [G loss: 0.624423]\n",
      "1328 [D loss: 14.663158] [G loss: 0.753064]\n",
      "1329 [D loss: 13.572482] [G loss: 1.064315]\n",
      "1330 [D loss: 13.852995] [G loss: 0.811074]\n",
      "1331 [D loss: 14.279388] [G loss: 1.065933]\n",
      "1332 [D loss: 15.153699] [G loss: 0.730528]\n",
      "1333 [D loss: 14.203972] [G loss: 1.043412]\n",
      "1334 [D loss: 16.278101] [G loss: 0.671335]\n",
      "1335 [D loss: 13.516100] [G loss: 1.129852]\n",
      "1336 [D loss: 13.631336] [G loss: 0.382570]\n",
      "1337 [D loss: 13.585727] [G loss: 0.529589]\n",
      "1338 [D loss: 16.624680] [G loss: 0.605155]\n",
      "1339 [D loss: 15.194315] [G loss: 0.982389]\n",
      "1340 [D loss: 13.274313] [G loss: 0.601016]\n",
      "1341 [D loss: 13.885707] [G loss: 1.466838]\n",
      "1342 [D loss: 15.015835] [G loss: 0.314201]\n",
      "1343 [D loss: 13.184019] [G loss: 0.997353]\n",
      "1344 [D loss: 13.301344] [G loss: 1.005929]\n",
      "1345 [D loss: 14.401408] [G loss: 0.721067]\n",
      "1346 [D loss: 16.568533] [G loss: 0.453979]\n",
      "1347 [D loss: 12.683536] [G loss: 1.126548]\n",
      "./dataset_2018_05_16/4/\n",
      "1348 [D loss: 14.828101] [G loss: 1.254369]\n",
      "1349 [D loss: 14.121000] [G loss: 0.800157]\n",
      "1350 [D loss: 14.488791] [G loss: 0.852571]\n",
      "1351 [D loss: 12.725204] [G loss: 0.860520]\n",
      "1352 [D loss: 14.725550] [G loss: 0.563573]\n",
      "1353 [D loss: 13.581398] [G loss: 0.735064]\n",
      "1354 [D loss: 14.824997] [G loss: 0.225899]\n",
      "1355 [D loss: 14.362314] [G loss: 0.767022]\n",
      "1356 [D loss: 13.569569] [G loss: 1.079486]\n",
      "1357 [D loss: 13.662125] [G loss: 0.618757]\n",
      "1358 [D loss: 13.577541] [G loss: 0.924013]\n",
      "1359 [D loss: 14.971448] [G loss: 0.820331]\n",
      "1360 [D loss: 13.657082] [G loss: 0.721295]\n",
      "1361 [D loss: 16.237841] [G loss: 0.473570]\n",
      "1362 [D loss: 13.824279] [G loss: 0.718776]\n",
      "1363 [D loss: 14.690487] [G loss: 0.690232]\n",
      "1364 [D loss: 13.647957] [G loss: 1.024584]\n",
      "1365 [D loss: 14.550809] [G loss: 0.450278]\n",
      "1366 [D loss: 13.271632] [G loss: 0.794413]\n",
      "1367 [D loss: 15.278307] [G loss: 1.056975]\n",
      "1368 [D loss: 14.423078] [G loss: 0.627298]\n",
      "1369 [D loss: 15.018196] [G loss: 0.909706]\n",
      "1370 [D loss: 14.723658] [G loss: 0.099914]\n",
      "1371 [D loss: 14.530085] [G loss: 0.565568]\n",
      "1372 [D loss: 13.290520] [G loss: 1.476183]\n",
      "1373 [D loss: 13.959756] [G loss: 1.301006]\n",
      "1374 [D loss: 14.727314] [G loss: 1.082157]\n",
      "1375 [D loss: 12.884818] [G loss: 1.144785]\n",
      "./dataset_2018_05_16/5/\n",
      "1376 [D loss: 14.883595] [G loss: 0.772388]\n",
      "1377 [D loss: 14.886093] [G loss: 0.855863]\n",
      "1378 [D loss: 15.949008] [G loss: 0.703145]\n",
      "1379 [D loss: 13.941333] [G loss: 0.306075]\n",
      "1380 [D loss: 14.457209] [G loss: 0.918128]\n",
      "1381 [D loss: 13.991564] [G loss: 0.766680]\n",
      "1382 [D loss: 13.485234] [G loss: 0.131090]\n",
      "1383 [D loss: 14.450827] [G loss: 0.501308]\n",
      "1384 [D loss: 14.710409] [G loss: 0.921611]\n",
      "1385 [D loss: 13.429726] [G loss: 0.909219]\n",
      "1386 [D loss: 13.819017] [G loss: 0.858543]\n",
      "1387 [D loss: 13.043028] [G loss: 0.865374]\n",
      "1388 [D loss: 13.333140] [G loss: 0.586012]\n",
      "1389 [D loss: 13.926531] [G loss: 0.792209]\n",
      "1390 [D loss: 14.131666] [G loss: 0.956287]\n",
      "1391 [D loss: 15.550189] [G loss: 0.615985]\n",
      "1392 [D loss: 15.806646] [G loss: 1.166069]\n",
      "1393 [D loss: 14.119757] [G loss: 1.258443]\n",
      "1394 [D loss: 14.389256] [G loss: 0.946004]\n",
      "1395 [D loss: 14.829548] [G loss: 1.196797]\n",
      "1396 [D loss: 13.467813] [G loss: 1.192358]\n",
      "1397 [D loss: 14.571227] [G loss: 0.720008]\n",
      "1398 [D loss: 15.305220] [G loss: 0.436125]\n",
      "1399 [D loss: 14.683868] [G loss: 1.095461]\n",
      "1400 [D loss: 14.424177] [G loss: 0.411785]\n",
      "1401 [D loss: 14.757603] [G loss: 0.851797]\n",
      "1402 [D loss: 13.416537] [G loss: 0.581370]\n",
      "1403 [D loss: 14.746323] [G loss: 1.082089]\n",
      "./dataset_2018_05_16/6/\n",
      "1404 [D loss: 14.574352] [G loss: 1.133089]\n",
      "1405 [D loss: 14.835419] [G loss: 0.396780]\n",
      "1406 [D loss: 15.098960] [G loss: 0.802331]\n",
      "1407 [D loss: 13.154695] [G loss: 0.869824]\n",
      "1408 [D loss: 14.908458] [G loss: 0.459146]\n",
      "1409 [D loss: 14.734963] [G loss: 0.645436]\n",
      "1410 [D loss: 13.980906] [G loss: 0.022397]\n",
      "1411 [D loss: 13.808233] [G loss: 0.746270]\n",
      "1412 [D loss: 14.055118] [G loss: 1.061624]\n",
      "1413 [D loss: 15.097618] [G loss: 0.562117]\n",
      "1414 [D loss: 14.671046] [G loss: 0.339957]\n",
      "1415 [D loss: 13.694348] [G loss: 0.776910]\n",
      "1416 [D loss: 14.843739] [G loss: 0.595719]\n",
      "1417 [D loss: 15.914196] [G loss: 0.880889]\n",
      "1418 [D loss: 15.327005] [G loss: 1.094109]\n",
      "1419 [D loss: 13.760937] [G loss: 0.454519]\n",
      "1420 [D loss: 14.051800] [G loss: 0.889815]\n",
      "1421 [D loss: 15.505894] [G loss: 0.936849]\n",
      "1422 [D loss: 13.620569] [G loss: 1.273828]\n",
      "1423 [D loss: 12.365654] [G loss: 0.528107]\n",
      "1424 [D loss: 14.732056] [G loss: 0.929088]\n",
      "1425 [D loss: 13.877222] [G loss: 0.627962]\n",
      "1426 [D loss: 14.961059] [G loss: 1.099620]\n",
      "1427 [D loss: 13.649096] [G loss: 0.068905]\n",
      "1428 [D loss: 12.571239] [G loss: 0.209408]\n",
      "1429 [D loss: 14.560750] [G loss: 0.414423]\n",
      "1430 [D loss: 14.682775] [G loss: 0.488142]\n",
      "1431 [D loss: 13.651610] [G loss: 0.410832]\n",
      "1432 [D loss: 13.035906] [G loss: 0.842584]\n",
      "./dataset_2018_05_16/7/\n",
      "1433 [D loss: 14.710949] [G loss: 0.692837]\n",
      "1434 [D loss: 15.284793] [G loss: 0.920617]\n",
      "1435 [D loss: 13.161249] [G loss: 0.620842]\n",
      "1436 [D loss: 14.507188] [G loss: 0.570971]\n",
      "1437 [D loss: 13.572712] [G loss: 0.809164]\n",
      "1438 [D loss: 15.736539] [G loss: 0.635828]\n",
      "1439 [D loss: 15.729215] [G loss: 0.731558]\n",
      "1440 [D loss: 15.397127] [G loss: 0.363957]\n",
      "1441 [D loss: 14.596202] [G loss: 1.020431]\n",
      "1442 [D loss: 13.831839] [G loss: 0.773433]\n",
      "1443 [D loss: 14.153572] [G loss: 0.628757]\n",
      "1444 [D loss: 14.322336] [G loss: 1.180143]\n",
      "1445 [D loss: 13.807802] [G loss: 0.357404]\n",
      "1446 [D loss: 13.092173] [G loss: 0.879246]\n",
      "1447 [D loss: 12.708016] [G loss: 0.718540]\n",
      "1448 [D loss: 13.800505] [G loss: 0.876361]\n",
      "1449 [D loss: 14.879857] [G loss: 0.425389]\n",
      "1450 [D loss: 14.009014] [G loss: 0.545034]\n",
      "1451 [D loss: 13.464791] [G loss: 1.224719]\n",
      "1452 [D loss: 13.699588] [G loss: 0.824355]\n",
      "1453 [D loss: 15.071287] [G loss: 1.128660]\n",
      "1454 [D loss: 15.400532] [G loss: 0.749110]\n",
      "1455 [D loss: 12.474076] [G loss: 0.684978]\n",
      "1456 [D loss: 15.230772] [G loss: 0.991644]\n",
      "1457 [D loss: 14.423335] [G loss: 0.770583]\n",
      "1458 [D loss: 14.195641] [G loss: 0.779693]\n",
      "1459 [D loss: 14.098374] [G loss: 0.410780]\n",
      "1460 [D loss: 14.706015] [G loss: 0.590875]\n",
      "./dataset_2018_05_16/8/\n",
      "1461 [D loss: 13.694650] [G loss: 1.113464]\n",
      "1462 [D loss: 11.932281] [G loss: 0.748989]\n",
      "1463 [D loss: 14.819736] [G loss: 0.524072]\n",
      "1464 [D loss: 14.895828] [G loss: 0.989115]\n",
      "1465 [D loss: 12.852200] [G loss: 0.532563]\n",
      "1466 [D loss: 14.958073] [G loss: 0.821844]\n",
      "1467 [D loss: 15.225265] [G loss: 0.713057]\n",
      "1468 [D loss: 13.647426] [G loss: 0.721419]\n",
      "1469 [D loss: 14.328897] [G loss: 1.096800]\n",
      "1470 [D loss: 15.078984] [G loss: 0.848083]\n",
      "1471 [D loss: 14.946614] [G loss: 1.178499]\n",
      "1472 [D loss: 14.105959] [G loss: 0.628564]\n",
      "1473 [D loss: 15.378448] [G loss: 0.580775]\n",
      "1474 [D loss: 14.222087] [G loss: 0.497983]\n",
      "1475 [D loss: 13.673451] [G loss: 1.140469]\n",
      "1476 [D loss: 15.168668] [G loss: 0.332597]\n",
      "1477 [D loss: 14.685707] [G loss: 1.540858]\n",
      "1478 [D loss: 15.223532] [G loss: 1.123822]\n",
      "1479 [D loss: 14.225512] [G loss: 1.135955]\n",
      "1480 [D loss: 12.810516] [G loss: 1.132636]\n",
      "1481 [D loss: 14.766287] [G loss: 1.107120]\n",
      "1482 [D loss: 15.187598] [G loss: 0.580686]\n",
      "1483 [D loss: 14.931759] [G loss: 0.534495]\n",
      "1484 [D loss: 13.705049] [G loss: 0.977876]\n",
      "1485 [D loss: 15.190866] [G loss: 0.535352]\n",
      "1486 [D loss: 13.968093] [G loss: 0.916646]\n",
      "1487 [D loss: 14.576589] [G loss: 1.108519]\n",
      "1488 [D loss: 16.260818] [G loss: 0.765855]\n",
      "./dataset_2018_05_16/9/\n",
      "1489 [D loss: 13.835711] [G loss: 0.632569]\n",
      "1490 [D loss: 15.797149] [G loss: 0.832695]\n",
      "1491 [D loss: 15.424973] [G loss: 0.406163]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1492 [D loss: 14.652448] [G loss: 0.502994]\n",
      "1493 [D loss: 14.268255] [G loss: 0.551621]\n",
      "1494 [D loss: 14.794157] [G loss: 1.286604]\n",
      "1495 [D loss: 12.150485] [G loss: 0.747723]\n",
      "1496 [D loss: 14.652231] [G loss: 0.860239]\n",
      "1497 [D loss: 14.874874] [G loss: 0.358973]\n",
      "1498 [D loss: 12.277978] [G loss: 0.941560]\n",
      "1499 [D loss: 15.868210] [G loss: 0.639853]\n",
      "1500 [D loss: 15.868701] [G loss: 0.556695]\n",
      "gan imaga2 :  (128, 128, 1)\n",
      "1501 [D loss: 14.854440] [G loss: 1.153178]\n",
      "1502 [D loss: 14.118792] [G loss: 0.879461]\n",
      "1503 [D loss: 15.127881] [G loss: 1.427886]\n",
      "1504 [D loss: 13.530153] [G loss: 1.216541]\n",
      "1505 [D loss: 13.851771] [G loss: 1.121686]\n",
      "1506 [D loss: 14.557508] [G loss: 0.910924]\n",
      "1507 [D loss: 15.275071] [G loss: 1.240774]\n",
      "1508 [D loss: 14.987554] [G loss: 1.062428]\n",
      "1509 [D loss: 14.514212] [G loss: 1.020890]\n",
      "1510 [D loss: 14.006155] [G loss: 0.539398]\n",
      "1511 [D loss: 14.207438] [G loss: 0.521599]\n",
      "1512 [D loss: 12.906631] [G loss: 1.103849]\n",
      "1513 [D loss: 13.172073] [G loss: 0.615024]\n",
      "1514 [D loss: 15.180863] [G loss: 0.542742]\n",
      "1515 [D loss: 14.928539] [G loss: 1.113724]\n",
      "1516 [D loss: 15.721960] [G loss: 0.471924]\n",
      "./dataset_2018_05_16/10/\n",
      "1517 [D loss: 14.660639] [G loss: 0.794039]\n",
      "1518 [D loss: 15.362216] [G loss: 0.957156]\n",
      "1519 [D loss: 13.605144] [G loss: 1.072328]\n",
      "1520 [D loss: 13.816969] [G loss: 0.422818]\n",
      "1521 [D loss: 15.500302] [G loss: 1.270228]\n",
      "1522 [D loss: 14.376959] [G loss: 1.089559]\n",
      "1523 [D loss: 15.088111] [G loss: 1.096615]\n",
      "1524 [D loss: 13.880118] [G loss: 0.666108]\n",
      "1525 [D loss: 15.168520] [G loss: 0.748850]\n",
      "1526 [D loss: 14.162256] [G loss: 0.679382]\n",
      "1527 [D loss: 14.336008] [G loss: 0.972555]\n",
      "1528 [D loss: 15.205620] [G loss: 0.848933]\n",
      "1529 [D loss: 13.509326] [G loss: 1.489630]\n",
      "1530 [D loss: 14.490961] [G loss: 0.668197]\n",
      "1531 [D loss: 13.897803] [G loss: 0.633446]\n",
      "1532 [D loss: 15.200264] [G loss: 0.753269]\n",
      "1533 [D loss: 14.024670] [G loss: 0.626019]\n",
      "1534 [D loss: 14.553015] [G loss: 0.752603]\n",
      "1535 [D loss: 14.636730] [G loss: 1.280763]\n",
      "1536 [D loss: 13.004667] [G loss: 0.495433]\n",
      "1537 [D loss: 14.729763] [G loss: 0.982313]\n",
      "1538 [D loss: 13.696888] [G loss: 0.739304]\n",
      "1539 [D loss: 14.926183] [G loss: 1.011240]\n",
      "1540 [D loss: 13.644921] [G loss: 0.635096]\n",
      "1541 [D loss: 14.322888] [G loss: 0.514372]\n",
      "1542 [D loss: 12.442502] [G loss: 1.110068]\n",
      "1543 [D loss: 14.506959] [G loss: 0.339418]\n",
      "1544 [D loss: 13.616150] [G loss: 0.581535]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-8feeaa65267c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;31m# x_train = loader.get_emg_datas(batch_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter/inspace/sang-min/myo_proejct/load_data.py\u001b[0m in \u001b[0;36mget_images\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;31m# image, label = self.load_image()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m             \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;31m# labels.append(label)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter/inspace/sang-min/myo_proejct/load_data.py\u001b[0m in \u001b[0;36mload_image\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m             image = cv2.imread(\n\u001b[1;32m    118\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_dir_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mimage_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 cv2.IMREAD_GRAYSCALE)\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0;31m# print(self.data_path + str(self.image_dir_index) + '/' + image_name + '-' + str(label) + '.png')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'  # 也可以使用 tensorflow\n",
    "# os.environ['THEANO_FLAGS']='floatX=float32,device=cuda,exception_verbosity=high'\n",
    "os.environ['THEANO_FLAGS'] = 'floatX=float32,device=cuda,optimizer=fast_compile'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"8\"\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "K.set_image_data_format('channels_last')\n",
    "\n",
    "import time\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, ZeroPadding2D, BatchNormalization, Input, LSTM, Concatenate, Dense, Dropout\n",
    "from keras.layers import Conv2DTranspose, Reshape, Activation, Cropping2D, Flatten, UpSampling2D, AveragePooling2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.activations import relu\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.datasets import mnist\n",
    "from urllib.request import urlretrieve\n",
    "from keras.optimizers import RMSprop, SGD, Adam, sgd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# from read_data import *\n",
    "a = %pwd\n",
    "print(a)\n",
    "\n",
    "%run /root/jupyter/inspace/sang-min/myo_proejct/load_data.py import DataLoader\n",
    "#%load load_data import DataLoader\n",
    "#from load_data import DataLoader\n",
    "\n",
    "\n",
    "'''\n",
    "Model structure\n",
    "\n",
    "Input : (100) Vector\n",
    "Output : (64, 64, 1) image\n",
    "\n",
    "Generator\n",
    "100 -> 256\n",
    "256 -> 16 x 16\n",
    "16 x 16 -> 32 x 32\n",
    "32 x 32 -> 64 x 64\n",
    "64 x 64 -> 128 x 128\n",
    "\n",
    "Discriminator\n",
    "128 x 128 -> 64 x 64\n",
    "64 x 64 -> 32 x 32\n",
    "32 x 32 -> 16 x 16\n",
    "16 x 16 -> 8 x 8\n",
    "8 x 8 -> 64\n",
    "64 -> 32 (16, 10)\n",
    "\n",
    "'''\n",
    "loader = DataLoader(data_path='./dataset_2018_05_16/', is_real_image=False)\n",
    "\n",
    "conv_init = RandomNormal(0, 0.02)\n",
    "gamma_init = RandomNormal(1., 0.02)\n",
    "\n",
    "def toggle_trainable(network, state):\n",
    "    network.trainable = state\n",
    "    \n",
    "    for layer in network.layers:\n",
    "        layer.trainable = state\n",
    "\n",
    "def generator_containing_discriminator(g, d):\n",
    "    model = Sequential()\n",
    "    model.add(g)\n",
    "    d.trainable = False\n",
    "    model.add(d)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def generative_model(noise_size, image_channel):\n",
    "    # lstm_layer = LSTM(80, input_shape=lstm_size)(lstm_input)\n",
    "    # _ = Concatenate(axis=-1)([lstm_layer, noise_input])\n",
    "    print(\" _ : \", noise_input)\n",
    "    _ = Dense(2*2*512, input_shape=(100,), activation='relu', bias_initializer='glorot_normal',\n",
    "              kernel_initializer='glorot_normal')(noise_input)\n",
    "    _ = BatchNormalization(axis=1, gamma_initializer=gamma_init)(_, training=1)\n",
    "    _ = Reshape((2, 2, 512), input_shape=(2*2*512,))(_)\n",
    "\n",
    "    _ = Conv2D(filters=256, kernel_size=(3, 3), strides=1, padding='same', kernel_initializer=conv_init)(_)\n",
    "    _ = BatchNormalization(axis=1, gamma_initializer=gamma_init)(_, training=1)\n",
    "    _ = LeakyReLU(alpha=0.2)(_)\n",
    "\n",
    "    _ = UpSampling2D()(_)\n",
    "    _ = Conv2D(filters=128, kernel_size=3, padding='same', kernel_initializer=conv_init)(_)\n",
    "    _ = BatchNormalization(axis=1, gamma_initializer=gamma_init)(_, training=1)\n",
    "    _ = LeakyReLU(alpha=0.2)(_)\n",
    "    _ = Dropout(0.3)(_)\n",
    "    \n",
    "    _ = UpSampling2D()(_)\n",
    "    _ = Conv2D(filters=64, kernel_size=3, padding='same',  kernel_initializer=conv_init)(_)\n",
    "    _ = BatchNormalization(axis=1, gamma_initializer=gamma_init)(_, training=1)\n",
    "    _ = LeakyReLU(alpha=0.2)(_)\n",
    "    #_ = Dropout(0.3)(_)\n",
    "\n",
    "    _ = UpSampling2D()(_)\n",
    "    _ = Conv2D(filters=32, kernel_size=3, padding='same', kernel_initializer=conv_init)(_)\n",
    "    _ = BatchNormalization(axis=1, gamma_initializer=gamma_init)(_, training=1)\n",
    "    _ = LeakyReLU(alpha=0.2)(_)\n",
    "    _ = Dropout(0.3)(_)\n",
    "    \n",
    "    _ = UpSampling2D()(_)\n",
    "    _ = Conv2D(filters=16, kernel_size=3, padding='same', kernel_initializer=conv_init)(_)\n",
    "    _ = BatchNormalization(axis=1, gamma_initializer=gamma_init)(_, training=1)\n",
    "    _ = LeakyReLU(alpha=0.2)(_)\n",
    "    #_ = Dropout(0.3)(_)\n",
    "    \n",
    "    _ = UpSampling2D()(_)\n",
    "    _ = Conv2D(filters=8, kernel_size=3, padding='same', kernel_initializer=conv_init)(_)\n",
    "    _ = BatchNormalization(axis=1, gamma_initializer=gamma_init)(_, training=1)\n",
    "    _ = LeakyReLU(alpha=0.2)(_)\n",
    "    _ = Dropout(0.3)(_)\n",
    "    \n",
    "    _ = UpSampling2D()(_)\n",
    "    _ = Conv2D(filters=image_channel, kernel_size=3, padding='same', kernel_initializer=conv_init)(_)\n",
    "    _ = Activation(activation='tanh')(_)\n",
    "\n",
    "    return Model(inputs=noise_input, outputs=_)\n",
    "\n",
    "\n",
    "def discriminative_model(image_size, image_channel):\n",
    "    _ = inputs = Input(shape=(image_size, image_size, image_channel))\n",
    "\n",
    "    _ = Conv2D(filters=32, kernel_size=(5, 5), border_mode='same', input_shape=(image_size,image_size, image_channel),\n",
    "               kernel_initializer=conv_init)(_)\n",
    "    _ = AveragePooling2D(pool_size=(2, 2), border_mode='valid')(_)\n",
    "    _ = LeakyReLU(alpha=0.2)(_)\n",
    "\n",
    "    _ = BatchNormalization(axis=1, gamma_initializer=gamma_init)(_, training=1)\n",
    "    _ = Conv2D(filters=64,kernel_size=(5, 5), border_mode='same', input_shape=(64, 64, 32),\n",
    "               kernel_initializer=conv_init)(_)\n",
    "    _ = AveragePooling2D(pool_size=(2, 2), border_mode='valid')(_)\n",
    "    _ = LeakyReLU(alpha=0.2)(_)\n",
    "\n",
    "    _ = BatchNormalization(axis=1, gamma_initializer=gamma_init)(_, training=1)\n",
    "    _ = Conv2D(filters=128, kernel_size=(5, 5), border_mode='same', input_shape=(32, 32, 64),\n",
    "               kernel_initializer=conv_init)(_)\n",
    "    _ = AveragePooling2D(pool_size=(2, 2), border_mode='valid')(_)\n",
    "    _ = LeakyReLU(alpha=0.2)(_)\n",
    "\n",
    "    _ = BatchNormalization(axis=1, gamma_initializer=gamma_init)(_, training=1)\n",
    "    _ = Conv2D(filters=256, kernel_size=(5, 5), border_mode='same', input_shape=(16, 16, 128),\n",
    "               kernel_initializer=conv_init)(_)\n",
    "    _ = AveragePooling2D(pool_size=(2, 2), border_mode='valid')(_)\n",
    "    _ = LeakyReLU(alpha=0.2)(_)\n",
    "\n",
    "    _ = BatchNormalization(axis=1, gamma_initializer=gamma_init)(_, training=1)\n",
    "    _ = Conv2D(filters=512, kernel_size=(5, 5), border_mode='same', input_shape=(8, 8, 256),\n",
    "               kernel_initializer=conv_init)(_)\n",
    "    _ = AveragePooling2D(pool_size=(2, 2), border_mode='valid')(_)\n",
    "    _ = LeakyReLU(alpha=0.2)(_)\n",
    "\n",
    "    _ = BatchNormalization(axis=1, gamma_initializer=gamma_init)(_, training=1)\n",
    "    _ = Conv2D(filters=1, kernel_size=(2, 2), strides=1, padding='same', input_shape=(4, 4, 512),\n",
    "               kernel_initializer=conv_init)(_)\n",
    "    # _ = LeakyReLU(alpha=0.2)(_)\n",
    "\n",
    "    outputs = Flatten()(_)\n",
    "    outputs = Dense(1, activation='sigmoid')(outputs)\n",
    "\n",
    "    return Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "lstm_size = (300, 16)\n",
    "noise_size = 100\n",
    "image_size = 128\n",
    "input_size = 100\n",
    "image_channel = 1\n",
    "learning_rate = 2e-4\n",
    "\n",
    "discriminator_optim = sgd(lr=0.01, momentum=0.9, nesterov=True)\n",
    "generator_optim = Adam(lr=1e-3)\n",
    "\n",
    "# lstm_input = Input(shape=lstm_size)\n",
    "noise_input = Input(shape=(noise_size,))\n",
    "real_image = Input(shape=(image_size, image_size, image_channel))\n",
    "\n",
    "net_g = generative_model(noise_size, image_channel)\n",
    "net_g.compile(loss='binary_crossentropy', optimizer=generator_optim)\n",
    "net_g.summary()\n",
    "\n",
    "net_d = discriminative_model(image_size, image_channel)\n",
    "net_d.compile(loss='binary_crossentropy', optimizer=discriminator_optim)\n",
    "net_d.summary()\n",
    "\n",
    "GAN = Sequential()\n",
    "GAN.add(net_g)\n",
    "GAN.add(net_d)\n",
    "GAN_optim = Adam(lr=1e-3)\n",
    "GAN.compile(loss='binary_crossentropy', optimizer=GAN_optim)\n",
    "GAN.summary()\n",
    "\n",
    "\n",
    "# net_g.compile(loss='binary_crossentropy', optimizer='SGD')\n",
    "\n",
    "\n",
    "'''\n",
    "loader = DataLoader(data_path='./MYO_Dataset_label/')\n",
    "\n",
    "emg = loader.load_emg_data()\n",
    "image, label = loader.load_image()\n",
    "\n",
    "print(emg.shape)\n",
    "print(image.shape, label)\n",
    "\n",
    "emg = loader.get_emg_datas(10)\n",
    "images, labels = loader.get_images(10)\n",
    "print(emg.shape, images.shape, labels.shape)\n",
    "\n",
    "'''\n",
    "\n",
    "epoch = 10000\n",
    "i = 0\n",
    "time_0 = time.time()\n",
    "batch_size = 32\n",
    "\n",
    "d_loss_history = []\n",
    "g_loss_history = []\n",
    "d_acc_history = []\n",
    "\n",
    "\n",
    "while i <= epoch:\n",
    "    # x_train = loader.get_emg_datas(batch_size)\n",
    "    images = loader.get_images(batch_size)\n",
    "    noise = np.random.normal(loc=0.0, scale=1.0,size=[batch_size, noise_size])\n",
    "\n",
    "    discrim_current_orig_train = images.copy()\n",
    "    discrim_current_noise =noise.copy()\n",
    "    discrim_generated_train = net_g.predict(discrim_current_noise)\n",
    "\n",
    "    # print(\"Starting to train the discriminator!\")\n",
    "    discrim_current_train = discrim_current_orig_train.copy()\n",
    "    discrim_current_labels = np.zeros(shape=[batch_size])\n",
    "    \n",
    "    # Original data samples have a label 1\n",
    "    for ix in range(batch_size):\n",
    "        if np.random.uniform(0.0, 1.0) < 0.95:\n",
    "            discrim_current_labels[ix] = np.random.uniform(0.7, 1.2)\n",
    "        else:\n",
    "            discrim_current_labels[ix] = np.random.uniform(0.0, 0.3)\n",
    "        \n",
    "    discriminator_loss_cur = net_d.train_on_batch(discrim_current_train, discrim_current_labels)\n",
    "\n",
    "    discrim_current_train = discrim_generated_train.copy()\n",
    "    discrim_current_labels = np.zeros(shape=[batch_size])\n",
    "    \n",
    "    # Generated samples have a label 0\n",
    "    for ix in range(batch_size):\n",
    "        if np.random.uniform(0.0, 1.0) < 0.95:\n",
    "            discrim_current_labels[ix] = np.random.uniform(0.0, 0.3)\n",
    "        else:\n",
    "            discrim_current_labels[ix] = np.random.uniform(0.7, 1.2)\n",
    "    discriminator_loss_cur += net_d.train_on_batch(discrim_current_train, discrim_current_labels)\n",
    "        \n",
    "    d_loss_history.append(discriminator_loss_cur)\n",
    "    #d_acc_history.append(100 * d_loss[1])\n",
    "\n",
    "    toggle_trainable(net_d, False)\n",
    "    noise2 = np.random.normal(loc=0.0, scale=1.0, size=[batch_size*2, 100])\n",
    "    gen_current_train = noise2.copy()\n",
    "    gen_current_labels = np.zeros(shape=[batch_size * 2])\n",
    "    # When we train the generator we want it to fool the discriminator so we use the opposite labels\n",
    "    # We use gen_current_labels[:] = 1 instead of using gen_current_labels[:] = 0\n",
    "    for ix in range(batch_size*2):\n",
    "        gen_current_labels[ix] = np.random.uniform(0.7, 1.2)\n",
    "    #print(\"Starting to train the generator!\")\n",
    "    generator_loss_cur = GAN.train_on_batch(gen_current_train, gen_current_labels)\n",
    "    g_loss_history.append(generator_loss_cur)\n",
    "    toggle_trainable(net_d, True)\n",
    "\n",
    "    print(\"%d [D loss: %f] [G loss: %f]\" % (i,discriminator_loss_cur, generator_loss_cur))\n",
    "    \n",
    "    if i % 500 == 0:\n",
    "        gan_image = net_g.predict(noise)\n",
    "        print(\"gan imaga2 : \", gan_image[0].shape)\n",
    "        cv2.imwrite('./output_image3/' + 'fake_image'+ str(i) + '.png', gan_image[0] * 127.5)\n",
    "        #cv2.imwrite('./output_image3/' + 'real_image'+ str(i) + '.png', images[0] * 127.5)\n",
    "        \n",
    "    i +=1;\n",
    "\n",
    "net_g.save_weights(\"./output_image3/g_model.h5\")\n",
    "net_d.save_weights(\"./output_image3/d_model.h5\")\n",
    "combined_model.save_weights(\"./output_image3/combined_model.h5\")\n",
    "\n",
    "g_model_json = net_g.to_json()\n",
    "with open(\"./output_image3/g_model.json\", \"w\") as json_file:\n",
    "    json_file.write(g_model_json)\n",
    "    \n",
    "d_model_json = net_d.to_json()\n",
    "with open(\"./output_image3/d_model.json\", \"w\") as json_file:\n",
    "    json_file.write(d_model_json)\n",
    "\n",
    "combined_model_json = combined_model.to_json()\n",
    "with open(\"./output_image3/combined_model.json\", \"w\") as json_file:\n",
    "    json_file.write(combined_model_json)\n",
    "\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "#print(\"save model \")\n",
    "plt.figure(1, figsize=(16, 8))\n",
    "plt.plot(d_loss_history)\n",
    "plt.ylabel('d_loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "\n",
    "plt.savefig('./output_image3/d_loss_history.png')\n",
    "\n",
    "#plt.show()\n",
    "\n",
    "plt.figure(2, figsize=(16, 8))\n",
    "plt.plot(g_loss_history)\n",
    "plt.ylabel('g_loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "\n",
    "plt.savefig('./output_image3/g_loss_history.png')\n",
    "\n",
    "#plt.show()\\\n",
    "\n",
    "plt.figure(3, figsize=(16, 8))\n",
    "plt.plot(d_acc_history)\n",
    "plt.ylabel('d_acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "\n",
    "plt.savefig('./output_image3/d_acc_history.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
